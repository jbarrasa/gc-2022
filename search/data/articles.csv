uri,body,title,date
https://dev.to/lirantal/securing-a-nodejs--rethinkdb--tls-setup-on-docker-containers,"We use RethinkDB at work across different projects. It isn’t used for any sort of big-data applications, but rather as a NoSQL database, which spices things up with real-time updates, and relational tables support.RethinkDB features an officially supported Node.js driver, as well as a community-maintained driver as well called rethinkdbdash which is promises-based, and provides connection pooling. There is also a database migration tool called rethinkdb-migrate that aids in managing database changes such as schema changes, database seeding, tear up and tear down capabilities.We’re going to use the official RethinkDB docker image from the docker hub and make use of docker-compose.yml to spin it up (later on you can add additional services to this setup).A fair example for docker-compose.yml:The compose file mounts a local tls directory as a mapped volume inside the container. The tls/ directory will contain our cert files, and the compose file is reflecting this.To setup a secure connection we need to facilitate it using certificates so an initial technical step:Important notes:Update the compose file to include a command configuration that starts the RethinkDB process with all the required SSL configurationImportant notes:You’ll notice there isn’t any cluster related configuration but you can add them as well if you need to so they can join the SSL connection: — cluster-tls — cluster-tls-key /tls/key.pem — cluster-tls-cert /tls/cert.pem — cluster-tls-ca /tls/ca.pemThe RethinkDB drivers support an ssl optional object which either sets the certificate using the ca property, or sets the rejectUnauthorized property to accept or reject self-signed certificates when connecting. A snippet for the ssl configuration to pass to the driver:Now that the connection is secured, it only makes sense to connect using a user/password which are not the default.To set it up, update the compose file to also include the — initial-password argument so you can set the default admin user’s password. For example:Of course you need to append this argument to the rest of the command line options in the above compose file.Now, update the Node.js driver settings to use a user and password to connect:Congratulations! You’re now eligible to “Ready for Production stickers.Don’t worry, I already mailed them to your address.",Securing a Node.js + RethinkDB + TLS setup on Docker containers,2017-08-21T18:41:06Z
https://dev.to/setevoy/neo4j-running-in-kubernetes-e4p,"In the previous post — Neo4j: graph database — run with Docker and Cypher QL examples — we’ve run the Neo4j database with в Docker.The next task is to run it in the Kubernetes cluster.Will use the Neo4j Community Edition, which will be running as a single-node instance as cluster ability for the Neo4j is available in the Enterprise version which is costs about 190.000 USD/year.For the Community Edition, we can apply a Helm chart — neo4j-community.Add its repository:Deploy to a custom namespace, in this example, it’s eks-dev-1-neo4j.With the --set accept its license, and add values for the StorageClass of its PersistentVolume, and add a password for the administrator access:Okay -  it’s deployed, check pods:Check its PersistentVolumeClaim:Check the corresponding PersistentVolume — pay attention, that we’ve created that PVC in the dynamic way, so it has the Reclaim policy set to Delete:Now, run port-forwarding to access the Neo4j server from your workstation:Check connection:The server is ready for work.Originally published at RTFM: Linux, DevOps и системное администрирование.",Neo4j: running in Kubernetes,2020-08-05T14:37:20Z
https://dev.to/divyanshutomar/introduction-to-redis-3m2a,"For a high traffic web service, it becomes a necessity for it to leverage some kind of caching mechanism. Caching is a way of storing computed data in memory so that future requests can be fulfilled right away. It also helps in avoiding any round trips to the data layer and computations on the application side if implemented with the right strategy. Redis and Memcached are two most popular memory-based stores available. In this post, we will explore some key concepts of Redis and go through some basic commands. Besides caching, Redis can be also used for other applications where there is a need for fast and frequent access to data.Redis is an in-memory data structure store supporting many data types like strings, hashes, sets, sorted sets, etc. Essentially, it is a key-value store.Every type of value in Redis is stored against a key that is binary safe and it can be anything from an empty string to long hash string. Every application should follow a predetermined schema for naming Redis keys to avoid any naming conflicts.Like every database, Redis contains a server for storing data in memory and clients which will execute commands against a server. For setting up the server on your local machine, I will recommend using Docker as it is easy to get started. If you have Docker daemon running on your machine, run the following command:This will run a Docker container with name local-redis on your localhost with port 6379. It uses the official Redis docker image to run the container.For the client, we can use the redis-cli for executing commands from a console on the Redis server. Open a new tab, and execute the following command to start a redis-cli session connected to local docker Redis server instance:Now we can start executing some basic Redis commands.Setting a value:Syntax: SET <key> <value> Example: SET firstname AlbertRetrieve a value:Syntax: GET <key> Example: GET firstnameCheck whether a key exists:Syntax: EXISTS <key>Deleting a key:A key can be removed along with its associated memory using: DEL <key> This is a synchronous blocking operation.A better way to remove keys will be to unlink them whose associated memory can be collected by a garbage collector later on. UNLINK <key>Setting a time to expire for key:EXPIRE <key> <seconds> PEXPIRE <key> <milliseconds>Setting a key with check for existence and expiry in one go:Syntax: SET <key> <value> <EX seconds>|<PX milliseconds> NX|XXNX - set only when a key does not exist. XX - set only when key already exists. EX - sets expire time for the key in seconds. PX - sets expire time for the key in milliseconds.Example:SET firstname Albert EX 10 NXThis will set the key firstname with string value ""Albert"" with an expiry time of 10 seconds only if the key does not exist.Increment or Decrement an integer value:Redis provides a convenient way to increment or decrement integer values that may be used as counters.Syntax: INCR <key> DECR <key> INCRBY <key> <increment value> DECRBY <key> <decrement value>Example: SET counter 4 INCRBY counter 6counter key will hold the value 4 initially, and after the second command, it will get incremented to 10.All the above mentioned commands just deal with storing and manipulating of string or integer values. There are other data structure values such as hashes, sets, bit arrays, etc. that can be used to solve complex problems.In a real-world application, you can use various programming language specific redis clients available for interacting with your Redis server from the application code.We will be writing a simple Node based application that exposes an endpoint for getting user info against an userid. A JSON file will act as our datastore to keep things as simple as possible.Now, make a redis helper file that forms an instance of the redis client connected to our Redis server. We are also writing some cache helper methods here for our route handlers.In main app file, write a route handler that accepts an userid against which the user info is to be retrieved. Next, form a unique redis key using the userid. This key will always be the same for every request for a given userid. Check for existence of this key in the Redis cache, and return the response if found.Else, we will query the data from our data source and set the response data to Redis cache before sending it back as a response.To have a look at the full code and tinker around with it, you can clone the following repository:An express application that demonstrates how redis can be utilized for caching data so that recurrent requests can be fulfilled right away.Congratulations! You have now learned the basics of Redis. If you'd like to take a deep dive, please have a look at the official redis docs.Thank you for following along and I hope this post would have been useful for you. Do follow me on Twitter to receive updates on such topics.",Introduction to Redis,2018-07-08T17:27:05Z
https://dev.to/zaiste/15-git-commands-you-may-not-know-4a8j,"Using Git may be intimidating at times. There are so many commands and details to learn. The documentation, however, while being immense, is still greatly accessible. Once you overcome the initial feeling of being overwhelmed, the things will start to fall into place.Here is a list of 15 Git commands that you may not know yet, but hopefully they will help you out on a journey to master this tool.—-amend allows to append staged changes (e.g. to add a forgotten file) to the previous commit. Adding —-no-edit on top of that will amend the last commit without changing its commit message. If there are no changes, -—amend will allow you to reword the last commit message.For more: git help commit-p (or —patch) allows to interactively select parts of each tracked file to commit. This way each commit contains only related changes.For more: git help addSimilar to git-add , you can use --patch option to interactively select parts of each tracked file to stash.For more: git help stashBy default, when stashing, the untracked files are not included. In order to change that bevahiour and include those files as well you need to use -u parameter. There is also -a (—all) which stashes both untracked and ignored files altogether, which is probably something you usually won’t need.--patch can be also used to selectively discard parts of each tracked file. I aliased this command as git discardFor more: git help checkoutThis command allows you to quickly switch to the previously checked out branch. On a general note - is an alias for the previous branch. It can be used with other commands as well. I aliased checkout to co so, it becomes just git co -If you are sure that all of your local changes can be discarded, you can use . to do it at once. It is, however, a good practice to always use checkout --patch.This command shows all staged changes (those added to the index) in contrast to just git diff which only shows changes in the working directory (without those in the index).For more: git help diffIf you want to rename the currently checked out branch, you can shorten this command to the following form:For more: git help branchIn order to rename a branch remotely, once you renamed your branch locally, you need to first remove that branch remotely and then push the renamed branch again.Rebasing may lead to conflicts, the following command will open all files which need your help to resolve these conflicts.This command will show a log with changes introduced by each commit from the last two weeks.Let's say you committed a file by mistake. You can quickly remove that file from the last commit by combining rm and commit --amend commands:This command will show all branches that contain a particular commit.For more: git help gcAlthough I like CLI a lot, I highly recommend checking Magit to further step up your Git game. It is one of best pieces of software I used.There is, also, a fantastic overview of recommended Git workflows available via help command. Be sure to read it thoroughly!",15 Git Commands You May Not Know,2019-03-31T19:44:16Z
https://dev.to/alexjitbit/removing-files-from-mercurial-history-1b15,"Sometimes you need to remove files from Mercurial completely, even from the history. For instance, if you have accidentally stored a sensitive file in the repo (some password, or an access key, or a code-signing certificate etc. etc.) or committed a huge binary. Here's how you remove it:You will need the hg convert command. This command is normally used to convert an SVN/GIT repository to Mercurial, but it can also be used to ""convert"" from Mercurial to Mercurial too.The cool thing about it is the --filemap option that specifies which files should be included in the conversion process and which should not.Add this code to .hgrc to enable the extension:Make sure all your teammates have pushed their local changes to the central repo (if any)Backup your repositoryCreate a ""map.txt"" file:Then run this command:NOTE: You have to use ""forward-slash"" in paths, even on Windows.Then wait and be patient. After a while you will have a new repo at c:\newrepo just without the filesIf you have some central hg-storage you will have to ""strip"" all your changesets and then ""push"" your local repostiroy again.For instance, if you use Bitbucket like us, go to ""Admin - Strip"". Enter ""0"" into the ""Revision"" box and this will remove all commits, but the ""wiki"" and ""issues"" areas will be preserved. Then - push the local repo back to Bitbucket again.There's an important catch though - because the above procedure affects history and re-assigns new ""ids"" to all your changesets, your teammates have to reclone the repo from the central repo again.",Removing files from Mercurial history,2019-04-20T16:30:22Z
https://dev.to/michelemauro/atlassian-sunsetting-mercurial-support-in-bitbucket-2ga9,"(I didn't expect to start writing here with a text like this. But since I'm looking for longer opinions that those that can be found on twitter, I thought it would be a good place to start, and a good topic to talk about.)The headline came to me via whatsapp from an ex-colleague: Atlassian sunsetting mercurial support in BitBucket. That was really a hard hit.For those who don't know, Mercurial was one of the first really distributed Distributed Version Control System that really took traction: it was started in 2005, and it is still heavily used: Mozilla, Vim and OpenJDK are some high-profile projects that host multiple years of development, and millions of lines of code in hg repositories.I started using Mercurial before git ever became mainstream, and because I was working with some colleagues outside the main office: we needed to collaborate on the same sources and, in that setting, we couldn't reach our central CVS. The ease of use of passing around ""boundles"" of commits, the simple commands and the almost impossibility of losing history made it possible to work for many months without even a central server, just passing files via skype.It was no match for what Git was at that time (early 2007): something Linus whipped up in a couple of weeks, not too stable, still optimized for the Linux use case and adventurous to use outside of it. Mercurial was already stable, well supported on windows, too, with a clear and well-thought model that was easy to understand and use.Twelve years later, git is still making releases with new commands to clarify and simplify some use cases, while Mercurial releases are about technology updates and new features. You can easily find (and probably need) tons of tutorials on how to use git and how to understand its peculiar concepts (like of course the excellent one here on DEV); meanwhile, every colleague I exposed to mercurial never needed any special training nor never lost work because of a detached head or a push in the wrong branch, no matter its previous level of expertise.Finally, in the last three-four years, the ""GitOps"" methods fueled the exponential growth of Git integrations, and we're now stuck in VHS land. Atlassian, of course, has a bottom line to keep in check and finally came to the conclusion that BitBucket is supporting one VCS too many, and the one used by ""1% of the users"" has to go. The irony of BitBucket being the first commercial solution for Mercurial is, of course, only adding sadness to this story.The options that Atlassian proposes are really underwhelming: a yet-to-be automated conversion service, some self-hosted pointers (Heptapod and Kallithea seem to be the most interesting; but they are a far cry from a supported commercial service with more than 10 years of experience), or do-it-yourself. And don't expect to keep PRs, comments, issues, metadata: no export path for them, nor conversion of project structure. When Atlassian pulls the plug in mid-2020, they'll be gone.That's a sad, sad turn of the story that will harm a really good project, a true Betamax of the VCS space.I wish however to thank a lot the original BitBucket team, that in 2008 believed in Mercurial and started its journey. Atlassian, while can be understood as a business entity, is still struggling in executing major moves like these while minimizing bad publicity and juggling worse karma; meme writers will just have more things to poke fun at.Of course, the main lesson here is, for the n-th time, be careful what you put on a cloud service. And you, what do you have on a cloud service? what part of the information about your projects (issues, discussions, decisions, permissions, people) will you be able to offload, reuse and recycle if your current provider pulls the plug on its service or part of it?",Atlassian sunsetting mercurial support in BitBucket,2019-08-23T20:51:49Z
https://dev.to/shirou/back-up-prometheus-records-to-s3-via-kinesis-firehose-54l4,"Because prometheus has not for long term storage, it will be erased at appropriate intervals. For long term data store, it would be preserved as being influxdb. However, AWS S3 is more easy place to manage.AWS Kinesis firehose has came to the Tokyo region in July 2017. By using kinesis firehose, we can automatically save records in S3.So I implemented a remote write adapter integration of prometheus which can sends records to Kinesis.https://github.com/shirou/prometheus_remote_kinesisThis is just for evaluation and not deployed to production, there may be some problems.It is necessary to build with go, but I omit it. It is easy to use multi stage build with Docker.Of course, you should set AWS credentials.I also put it in the docker hub.https://hub.docker.com/r/shirou/prometheus_remote_kinesis/It should start with such feeling.Set the remote write setting of prometheus.yml as follows. It is important to add a - before the url to make it a sequence.The settings of kinesis and kinesis forehose are omitted.The setting is over with the above. As time goes on the logs are generated more and more in s3.The data sent to kinesis was made into JSON format like this.In Timeseries of prometheus, multiple samples can be stored for one records. However, since I do not want to increase the hierarchy much, I flatten it so that I create one record for each sample. As it was impossible to truly label, it is on the map. In addition, it assumes use from Athena or S3 SELECT, and it makes it with new line (JSON-LD). I tried to send it to kinesis with gzip compressed, but I removed currently because my t2.small uses a CPU too much. In addition, it sends it by PutRecords by every 500 records. As it will be buffered, it may be lost if remoe_kinesis die. Graceful shutdown is implemented, though.Since the write request from prometheus comes with snappy compression + protobuf, it may be the fastest way to transfer to kinesis with its byte sequence as it is, but this will be difficult to handle it later .I created remote storage integration to save to s3 via AWS Kinesis for long term log preset of prometheus.I have not deployed to production environment, so there may be problems. Another problem is reading from S3. But it is just a plain JSON format, I think it is easy to convert if necessary. Although it is possible to read directory from promehteus with remote read storage integration, but it probably not good performance.Oh, AlpacaJapan is recruiting highly acclaimed people who will do this around. Please contact Twitter @r_rudi",Back up prometheus records to s3 via kinesis firehose,2017-12-23T08:07:19Z
https://dev.to/ionic/farewell-phonegap-reflections-on-my-hybrid-app-development-journey-10dh,"Adobe has officially announced the shutdown of PhoneGap and PhoneGap Build.In the context of the hybrid app development world, this is the end of an era. It's certainly the case for me: it sped up my transition from .NET to web development, and ultimately led to me landing a wonderful role at Ionic. A heartfelt thanks to the team(s) at Adobe and those in the community who supported me along the way!PhoneGap has had such a positive impact on my career. Here's my hybrid app development journey.It was 2010, and I had just bought my first smartphone - a clunky Android device. I was .NET developer building tools and WinForm desktop apps for a SaaS company. That was fun, but my work was only used by a handful of corporate clients. This Android phone had potential - I could build an ""app"" and reach anyone in the world via the app marketplaces? Sign me up!I learned Java in college, so Android development was the obvious choice to learn. I bought a beginner Android book, ""Hello, Android"", and got to work. The dev experience was brutal to say the least. Between confusing Eclipse IDE errors and trying to understand the ins and outs of mobile development, I almost gave up multiple times.I pushed through, and in February 2011 released the app. Frustrated that Netflix movies would expire and be removed from my instant queue without notice, I explored my options. I discovered that Netflix had an open API, and while not used on the site, every movie was assigned a ""movie availability"" (expiration) date!FixMyQ was born: it displayed each movie in your Instant Queue along with its expiration date. Optionally, with one button tap, you could rearrange your entire queue by the movies expiring next. In practice, after deciding to watch something on Netflix, you could pull up my app first then pick based on what was expiring soonest:Despite being super ugly (ha), the app worked quite well and was decently popular.The app was doing well, but I was missing out on a huge audience: iOS users. I quickly realized that targeting iOS meant that I had to completely rewrite the app - yikes! Fortunately, there was another way: PhoneGap.Through my day job work and attending developer conferences, I noticed this thing called ""JavaScript"" was skyrocketing in popularity. I began to actively seek out opportunities at work to use it - landing on ASP.NET MVC, jQuery, and Knockout.js. I don't remember exactly how I found PhoneGap, but I loved the idea of ""write once, run everywhere"": targeting web, iOS, and Android with one code base.Additionally, their beliefs, goals, and philosophy really struck a chord. The team recognized that the web was not a first class development platform, but they fully believed it could be, laying out a strong vision for its future.What really stood out at the time (and still does) was this line:""The ultimate purpose of PhoneGap is to cease to exist.""To this day, I've yet to see another project put that front and center! It made sense, though: they were committed to the ""standardization of the web as a platform.""I was convinced and started building FixMyQ for iOS using PhoneGap 1.2.0. Unfortunately, I didn't get very far: Netflix deprecated then eventually shut down their open API - effectively killing the app. It was a great first mobile app project though and made for a fun retrospective.(Not so) hilariously, in 2020 Netflix still acts like movies don't expire. C'mon, Netflix!Despite shutting down my first app, I was excited by PhoneGap's potential and got to work right away on a new app idea. Work had just bought everyone a Fitbit device. I was also in the WeightWatchers program, so I wondered what it would take to integrate them together. A few months later, Fitwatchr was born and thanks to PhoneGap, I created iOS, Android, and Windows Phone apps all from one code base:Besides improving my web development skills, Fitwatchr was my first foray into becoming somewhat of an entrepreneur: in order to improve app sales, I learned so much about marketing, sales, and product development, ultimately earning thousands of dollars over a ~5 year timespan. As the app started making waves, I partnered with my good friend David Lapekas for design and marketing help - he was absolutely critical to my apps' success.You might say I was ""hooked on hybrid!""My next app scratched another itch. I love craft beer and had gotten really into tracking beer tastings with Untappd (another PhoneGap/Cordova - and later, Ionic Framework - app!). Their app was great, but didn't work well in offline scenarios (such as beer festivals or inside crowded brewery tasting rooms) where cell service was weak or wifi non-existent. With BeerSwift, you can queue up the beers you're drinking, rate them, then check them all into Untappd with one button tap (once you're back online):These apps were so much fun to build. I worked on them during the days of Angular 1, but was honestly scared off by how complex it seemed. So instead, I opted for a simpler stack: Vanilla HTML/CSS/JavaScript paired with jQuery, KendoUI Mobile for UI components, and Knockout.js for declarative bindings and automatic UI refresh.As you can tell from those screenshots, the apps look much better than my original Android app, but the UI still has some rough edges. Someday I'll rewrite them using Ionic Framework UI components...While PhoneGap makes it easy to create an app that runs on all platforms, in practice managing each platform is challenging, especially as a solo developer. There are nuances to each one as well as headaches with security profiles and certificates (cough cough iOS!). Enter Adobe's PhoneGap Build service, which let you build your PhoneGap-based Android, iOS, and Windows Phone apps in the cloud. It was incredibly successful as one of the early attempts at Mobile CI/CD since you could avoid wrestling with the challenges of native IDEs and tooling. Everyone in the PhoneGap community embraced it: solo devs, teams, and large companies.After gaining lots of experience with the service, I began sharing various tips and tricks on my personal blog. I'm particularly proud of the ""Cut Your PhoneGap Build App Size In Half With This One Weird Trick!"" post - one of my first attempts at ""marketing."" 😂It was rewarding to share what I'd learned with the community. I kept plugging away on both app development and blogging. From there, I decided to give public speaking a try, presenting a talk on hybrid app development at That Conference 2014.By 2015, hybrid app development had become far less niche and I had built up a lot of experience with several successful apps under my belt. I looked for my next challenge and settled on creating a video course on PhoneGap Build. With only a small blog audience, I turned to Pluralsight. I was a long-time fan - they are known for their high quality courses and popular authors. After a brief audition, I was in! You can read about that 2 year journey (yeah) here. It was incredibly challenging with lots of ups and downs, but in the end, I pulled it off.So unbelievably stoked! A long journey comes to an end. My @pluralsight course has been released! Cheers to a wonderful editorial team and @nursingaround 's support. #PhoneGap #Cordova #MobileDevA post shared by Matt Netkow (@dotnetkow ) on Jul 11, 2017 at 8:53pm PDTThe Pluralsight course was not a major hit by any means, but it was definitely a personal success: I learned basic video editing and production, and improved my writing and speaking skills along the way - all skills I would eventually use regularly in my DevRel role at Ionic.At some point during the development of my PhoneGap apps, I became frustrated trying to create the variety of icons and splash screens. Besides the act of creating them (I'm certainly no designer!), generating them for each platform and dimension was tedious. I'm not entirely sure, but I believe this was the first time I learned about Ionic: I stumbled upon a blog post of theirs on automating icon/splash screen generation.I created an Ionic account just to generate those images for free with the Ionic CLI (they were originally built in the cloud). Thanks, Ionic! 😬Little did I know where I'd end up someday...As part of the efforts to promote my PhoneGap Build Pluralsight course, I reached out to the PhoneGap team and asked about writing a post for the official blog. They graciously accepted, no doubt largely due to my course and personal PhoneGap blog posts, so I wrote ""Hybrid Mobile Apps are Overtaking Native."" This was a fun one: I covered the most popular concerns about hybrid app development from a fresh 2017 perspective: performance, design, frameworks, and tooling.By then, I was a regular reader of the Ionic blog and used (borrowed?) an image of theirs for the post (Thanks again, Ionic!). It was well received and led to a bunch of native developers leaving many ""spirited"" comments. Ha!Later, after the iPhone X was released, I struggled to update my PhoneGap apps to support the infamous ""notch."" I eventually figured out a general solution then wrote another guest post for the PhoneGap blog. To date, ""Displaying a PhoneGap App Correctly on the iPhone X"" is my highest viewed piece of writing ever with over 223,000 views (the notch still confuses developers to this day!).My final post for the PhoneGap blog, ""PhoneGap Devs: It’s Time to Embrace a UI Framework"" was a clear call to action to the community: pick a UI framework so that you can focus on building your app instead of dealing with various mobile issues (like the iPhone notch!). By that time, I was working for Ionic so naturally the article focused on the Ionic Framework.Huge thanks to the PhoneGap team for allowing me to guest on the blog!By the time 2018 rolled around, I was even deeper into web development, working regularly with Angular 2 and .NET Core at my day job. Angular, while initially challenging to learn, was a breathe of fresh air compared to my now-aging ""PhoneGap stack.""One night, I saw a tweet from the Ionic team:Calling all product champions! Do you have a technical background, but enjoy being the face and voice of the product, rather than sitting behind the scenes? Ionic is looking for a Senior Product Evangelist to join the team! Position can be remote! https://t.co/lHQo6OHrcJThe timing was simply incredible: at that moment I was taking a break from packing up my apartment. I planned to move back to Madison, Wisconsin that Summer, where Ionic headquarters is located. Leveraging my PhoneGap guest blog posts, Pluralsight course, and hybrid app experience, I landed the role as a Product Evangelist/Developer Advocate. See the full story here.When I started building PhoneGap apps, I had no idea what it would lead to. Hybrid app development has been such an incredibly rewarding career path. After years of hard work, some luck along the way, and support from an amazing community, I'm grateful to work on hybrid app development fulltime now, and for such an awesome company as Ionic.So as you can see, PhoneGap changed my life for the better. I owe a lot of my career to this amazing technology and the people that built it. But enough about me. 😀Did PhoneGap succeed in its mission to make the web platform a first-class citizen?""We believe in a web open to everyone to participate however they will. No locked doors. No walls.""Broadly speaking, PhoneGap absolutely succeeded: as pioneers of hybrid app development, they ""solved"" cross-platform app development challenge while also being incredibly influential in making the web a first-class development platform.In the time since it was created - over 12 years ago - we've seen the web platform explode in popularity: the vast majority of developers are web developers and many folks new to software development learn web dev first since it's so accessible and open.Sure, the web platform isn't perfect, but it's come a long way and will continue to evolve. It has matured a lot over the past few years, from modern JavaScript (ES6 and beyond) to package managers like npm, to built-in cross-platform browser APIs that provide rich user experiences, to the rise of Progressive Web Apps (PWAs) that fill the ""gap"" in ""PhoneGap.""Now, all of us at Ionic are ready to carry the torch as the modern leader of cross-platform hybrid app development. Our native runtime tool Capacitor, as a spiritual successor to PhoneGap, offers a modern, web-first approach to hybrid and is backward compatible with PhoneGap.Thank you to Adobe and the PhoneGap team for their hard work over the years and helping so many developers embrace web development. Long live the web platform!","Farewell, PhoneGap: Reflections on my Hybrid App Development Journey",2020-08-13T12:13:56Z
https://dev.to/rootsami/rancher-kubernetes-on-openstack-using-terraform-1ild,"In this article we will walk through creating complete infrastructure pieces on OpenStack that are needed to have a fully provisioned Kubernetes cluster using Terraform and Rancher2. In addition to integration with cloud-provider-openstack and cinder-csi-pluginRKE configuration can be adjusted and customized in rancher2.tf, you can check the provider documentation at rancher_cluster NOTE: It is really important to keep kubelet extra_args for the external cloudprovider in order to integrate with cloud-provider-openstackRun terraform init to initialize a working directory containing Terraform configuration files.To apply the creation of the environment, Run terraform apply --auto-approve and wait for the output after all resources finish the creationUp to this point, use the rancher_url from above output and login to rancher instance with username admin and password defined in rancher_admin_password. Wait for all kubernetes nodes to be discovered, registered, and active.As you may notice, that all the nodes have a taint node.cloudprovider.kubernetes.io/uninitialized. The usage of --cloud-provider=external flag to the kubelet makes it waiting for the clouder-provider to start the initialization. This marks the node as needing a second initialization from an external controller before it can be scheduled work.Up to this point, openstack-cloud-controller-manager and cinder-csi-plugin have been deployed, and they're able to obtain valuable information such as External IP addresses and Zone info.Also, as shown in the nodes tab, All nodes are active and labeled by openstack zones.When it comes to scalability with IaaC (infrastructure-as-a-code), it becomes so easy to obtain any desired state in less consumed efforts and time. All you have to do is to change the number of nodes count_master or count_worker_nodes and run terraform apply again For example, let's increase the number of count_worker_nodes by 1 A few minutes later, after refreshing states and applying updates:Couple of minutes for the new node to be registeredNOTE: Scaling down the cluster could be made by decreasing the number of nodes in terrafrom.tfvars. Node gets deleted, moreover cloud-provider-openstack detects that and removes it from the clusterTo clean up all resources created by this terraform, Just run terraform destroyTerraform manifests to create e2e production-grade Kubernetes cluster on top of cloud providersThis repo is intended to be for creating complete infrastructure pieces on OpenStack that are needed to have a fully provisioned Kubernetes cluster using Terraform and Rancher2. In addition to integration with cloud-provider-openstack",Rancher Kubernetes on Openstack using Terraform,2020-05-27T03:33:00Z
https://dev.to/jignesh_simform/comparing-mongodb--mysql-bfa,"Imagine finding a DBMS that aligns with tech goals of your organization. Pretty exciting, right?Relational databases held the lead for quite a time. Choices were quite obvious: MySQL, Oracle or MS SQL, to mention a few. Though times have changed pretty much with the demand for more diversity and scalability, haven't they?There are many alternatives in the market to choose from, though I don’t want you to get all confused again. So how about a faceoff between two dominant solutions that are close in popularity?MongoDB vs MySQL?Both of these are some of the most popular open-source database software.On that note, let’s get started.One of the best things about MongoDB is that there are no restrictions on schema design. You can just drop a couple of documents within a collection and it isn’t necessary to have any relations between those documents. The only restriction with this is supported data structures.But due to the absence of joins and transactions (which we will discuss later), you need to frequently optimize your schema based on how the application will be accessing the data.Before you can store anything in MySQL, you need to clearly define tables and columns, and every row in the table should have the same column.And because of this, there isn’t much space for flexibility in the manner of storing data if you follow normalization.For example, if you run a bank, its information can be added to the table named ‘account’ as follows:This is how MySQL stores the data. As you can see, the table design is quite rigid and it is not easily changeable. MongoDB stores the data in the JSON type manner as described below:Such documents can be stored in a collection as well.MongoDB creates schemaless documents which can store any information you want though it may cause problems with data consistency. MySQL creates a strict schema-template and hence it is bound to make mistakes.MongoDB uses an unstructured query language. To build a query in JSON documents, you need to specify a document with properties you wish the results to match.It is typically executed using a very rich set of operators that are linked to each other using JSON. MongoDB treats each property as having an implicit boolean AND. It natively supports boolean OR queries, but you must use a special operator ($or) to achieve it.MySQL uses the structured query language SQL to communicate with the database. Despite its simplicity, it is indeed a very powerful language which consists mainly of two parts: data definition language (DDL) and data manipulation language (DML).Let’s have a quick comparison.MongoDB doesn’t support JOIN — at least, it has no equivalent. On the contrary, it supports multi-dimensional data types such as arrays and even other documents. The placement of one document inside another is known as embedding.One of the best parts about MySQL is the JOIN operations. To put it in simple terms, JOIN makes the relational database relational. JOIN allows the user to link data from two or more tables in a single query with the help of single SELECT command.For example, we can easily obtain related data in multiple tables using a single SQL statement.This should provide you with an account number, first name, and the respective branch.One single main benefit it has over MySQL is its ability to handle large unstructured data. It is magically faster because it allows users to query in a different manner that is more sensitive to workload.Developers note that MySQL is quite slower in comparison to MongoDB when it comes to dealing with large databases. It is unable to cope with large and unstructured amounts of data.As such, there is no “standard” benchmark that can help you with the best database to use for your needs. Only your demands, your data, and infrastructure can tell you what you need to know.Let’s look at a general example to understand the speed of MySQL and MongoDB in accordance with various functions.Measurements have been performed in the following cases:MySQL 5.7.9MongoDB 3.2.0Each of these has been tested on a separate m4.xlarge Amazon instance with Ubuntu 14.4 x64 and default configurations; all tests were performed for 1,000,000 records.It is evident from the above graph that MongoDB takes way more lesser time than MySQL for same commands.MongoDB uses a role-based access control with a flexible set of privileges. Its security features include authentication, auditing, and authorization.Moreover, it is also possible to use Transport Layer Security (TLS) and Secure Sockets Layer (SSL) for encryption purposes. This ensures that it is only accessible and readable by the intended client.MySQL uses a privilege-based security model. This means it authenticates a user and facilitates it with user privileges on a particular database such as CREATE, SELECT, INSERT, UPDATE, and so on.But it fails to explain why a given user is denied specific access. On the transport layer, it uses encrypted connections between clients and the server using SSL.When to Use MongoDB or MySQL? This infographic explains when you'd use MongoDB over MySQL and vice versa.To answer the question, “Why I should use X over Y?” you need to take into consideration your project goals and many other things.MySQL is highly organized for its flexibility, high performance, reliable data protection, and ease of managing data. Proper data indexing can resolve your issue with performance, facilitate interaction and ensure robustness.But if your data is not structured and complex to handle, or if predefining your schema is not coming easy for you, you should better opt for MongoDB. What’s more, if you're required to handle a large volume of data and store it as documents, MongoDB will help you a lot!The result of the faceoff: One isn’t necessarily better than the other. MongoDB and MySQL both serve in different niches.We have published an updated version of this post here MongoDB vs MySQL: A Comparative Study on Databases. If you've more suggestions up your sleeve, kindly comment.",Comparing MongoDB & MySQL,2017-12-05T12:19:07Z
https://dev.to/nipeshkc7/dynamodb-the-basics-360g,"Recently I've started looking at Serverless architecture inorder to move away from the monolithic architecture. While exploring AWS free tier, Amazon DynamoDB stood out to me. As a NOSQL database with free 25 GB of storage, I could use it for my personal projects basically for free.So I've been reading up on AWS docs on the basics of DynamoDB, below listed are some of the basic terminology I have gotten to know about DynamoDB:Strange name, given DynamoDB is NoSQL, However, it serves as a good mental model, and helps understand how data is grouped together. These 'tables' consists of a collection of 'items'.Items can be thought of as 'rows' in the tables, But unlike traditional tables, it does not have or need a specific schema. Aside from the primary key, it can contain any number of 'key-value' relations, and can also store nested objects upto 32 levels deep.This is a simple one, Primary key uniquely identifies the 'item' in the table, it can be a single attribute or a combination of attributes.The image above is from AWS docsIn this specific case, 'People' is the table, which consists of a collection of items, And 'PersonID' is the primary key which uniquely identifies the item. Here, besides 'PersonID', other key value attributes can be anything, even nested items.So AWS DynamoDB is an awesome free data storage option, however NoSQL Databases has its own use cases as does Relational Databases. So it is important to figure out what kind of storage option is best for your specific project before going forward.Thanks for reading.P.S. Please follow me on twitter @Nipeshkc7",DynamoDB: the basics,2020-06-02T04:09:36Z
https://dev.to/heroku/postgres-is-underrated-it-handles-more-than-you-think-4ff3,"Thinking about scaling beyond your Postgres cluster and adding another data store like Redis or Elasticsearch? Before adopting a more complex infrastructure, take a minute and think again. It’s quite possible to get more out of an existing Postgres database. It can scale for heavy loads and offers powerful features which are not obvious at first sight. For example, its possible to enable in-memory caching, text search, specialized indexing, and key-value storage.After reading this article, you may want to list down the features you want from your data store and check if Postgres will be a good fit for them. It’s powerful enough for most applications.As Fred Brooks put it in The Mythical Man-Month: ""The programmer, like the poet, works only slightly removed from pure thought-stuff. [They] build castles in the air, from air, creating by exertion of the imagination.""Adding more pieces to those castles, and getting lost in the design, is endlessly fascinating; however, in the real world, building more castles in the air can get in your way. The same holds true for the latest hype in data stores. There are several advantages to choosing boring technology:Although it can be managed by thoughtful design, adding multiple datastores does increase complexity. Before exploring adding additional datastores, it's worth investigating what additional features your existing datastores can offer you.Many people are unaware that Postgres offers way more than just a SQL database. If you already have Postgres in your stack, why add more pieces when Postgres can do the job?There’s a misconception that Postgres reads and writes from disk on every query, especially when users compare it with purely in-memory data stores like Redis.Actually, Postgres has a beautifully designed caching system with pages, usage counts, and transaction logs. Most of your queries will not need to access the disk, especially if they refer to the same data over and over again, as many queries tend to do.The shared_buffer configuration parameter in the Postgres configuration file determines how much memory it will use for caching data. Typically it should be set to 25% to 40% of the total memory. That’s because Postgres also uses the operating system cache for its operation. With more memory, most recurring queries referring the same data set will not need to access the disk. Here is how you can set this parameter in the Postgres CLI:Managed database services like Heroku offer several plans where RAM (and hence cache) is a major differentiator. The free hobby version does not offer dedicated resources like RAM. Upgrade when you’re ready for production loads so you can make better use of caching.You can also use some of the more advanced caching tools. For example, check the pg_buffercache view to see what’s occupying the shared buffer cache of your instance. Another tool to use is the pg_prewarm function which comes as part of the base installation. This function enables DBAs to load table data into either the operating system cache or the Postgres buffer cache. The process can be manual or automated. If you know the nature of your database queries, this can greatly improve application performance.For the really brave at heart, refer to this article for an in-depth description of Postgres caching.Elasticsearch is excellent, but many use cases can get along just fine with Postgres for text searching. Postgres has a special data type, tsvector, and a set of functions, like to_tsvector and to_tsquery, to search quickly through text. tsvector represents a document optimized for text search by sorting terms and normalizing variants. Here is an example of the to_tsquery function:You can sort your results by relevance depending on how often and which fields your query appeared in the results. For example, you can make the title more relevant than the body. Check the Postgres documentation for details.Postgres provides a powerful server-side function environment in multiple programming languages.Try to pre-process as much data as you can on the Postgres server with server-side functions. That way, you can cut down on the latency that comes from passing too much data back and forth between your application servers and your database. This approach is particularly useful for large aggregations and joins.What’s even better is your development team can use its existing skill set for writing Postgres code. Other than the default PL/pgSQL (Postgres’ native procedural language), Postgres functions and triggers can be written in PL/Python, PL/Perl, PL/V8 (JavaScript extension for Postgres) and PL/R.Here is an example of creating a PL/Python function for checking string lengths:Extensions are to Postgres what plug-ins mean in many applications. Suitable use of Postgres extensions can also mean you don’t have to work with other data stores for extra functionality. There are many extensions available and listed on the main Postgres website.PostGIS is a specialized extension for Postgres used for geospatial data manipulation and running location queries in SQL. It’s widely popular among GIS application developers who use Postgres. A great beginner’s guide to using PostGIS can be found here.The code snippet below shows how we are adding the PostGIS extension to the current database. From the OS, we run these commands to install the package (assuming you are using Ubuntu):After that, log in to your Postgres instance and install the extension:If you want to check what extensions you have in the current database, run this command:The Postgres hstore extension allows storing and searching simple key-value pairs. This tutorial provides an excellent overview of how to work with hstore data type.There are two native data types for storing semi-structured data in Postgres: JSON and XML. The JSON data type can host both native JSON and its binary form (JSONB). The latter can significantly improve query performance when it is searched. As you can see below, it can convert JSON strings to native JSON objects:If you’re considering switching off Postgres due to performance reasons, first see how far you can get with the optimizations it offers. Here we'll assume you've done the basics, like creating appropriate indexes. Postgres offers many advanced features, and while the changes are small they can make a big difference, especially if it keeps you from complicating your infrastructure.Avoid unnecessary indexes. Use multi-column indexes sparingly. Too many indexes take up extra memory that crowd out better uses of the Postgres cache, which is crucial for performance.Using a tool like EXPLAIN ANALYZE might surprise you by how often the query planer actually chooses sequential table scans. Since much of your table’s row data is already cached, oftentimes these elaborate indexes aren’t even used.That said, if you do find slow queries, the first and most obvious solution is to see if the table is missing an index. Indexes are vital, but you have to use them correctly.A partial index can save space by specifying which values get indexed. For example, you want to order by a user’s signup date, but only care about the users who have signed up:Choosing the right index for your data can improve performance. Here are some common index types and when you should use each one.There are legitimate cases for adding another datastore beyond Postgres.Some data stores give you data types that you just can’t get on Postgres. For example, the linked list, bitmaps, and HyperLogLog functions in Redis are not available on Postgres.At a previous startup, we had to implement a frequency cap, which is a counter for unique users on a website based on session data (like cookies). There might be millions or tens of millions of users visiting a website. Frequency capping means you only show each user your ad once per day.Redis has a HyperLogLog data type that is perfect for a frequency cap. It approximates set membership with a very small error rate, in exchange for O(1) time and a very small memory footprint. PFADD adds an element to a HyperLogLog set. It returns 1 if your element is not in the set already, and 0 if it is in the set.If you’re in a situation with many pub-sub events, jobs, and dozens of workers to coordinate, you may need a more specialized solution like Apache Kafka. LinkedIn engineers originally developed Kafka to handle new user events like clicks, invitations, and messages, and allow different workers to handle message passing and jobs to process the data.If you have a real-time application under heavy load with more than ten searches going on at a time, and you need features like autocomplete, then you may benefit more from a specialized text solution like Elasticsearch.Redis, Elasticsearch, and Kafka are powerful, but sometimes adding them does more harm than good. You may be able to get the capabilities you need with Postgres by taking advantage of the lesser-known features we’ve covered here. Ensuring that you are getting the most out of Postgres can save you time and help you avoid added complexity and risks.To save even more time and headaches, consider using a managed service like Heroku Postgres. Scaling up is a simple matter of adding additional follower replicas, high availability can be turned on with a single click, and Heroku operates it for you. If you really need to expand beyond Postgres, the other data stores that we mentioned above, such as Redis, Apache Kafka and Elasticsearch, can all be easily provisioned on Heroku. Go ahead and build your castles in the air―but anchor them to a reliable foundation, so you can dream about a better product and customer experience.For more information on Postgres, listen to Cloud Database Workloads with Jon Daniel on Software Engineering Daily.",Postgres Is Underrated—It Handles More than You Think,2019-10-09T15:04:20Z
https://dev.to/dmfay/the-ultimate-postgres-vs-mysql-blog-post-1l5f,"I should probably say up front that I love working with Postgres and could die happy without ever seeing a mysql> prompt again. This is not an unbiased comparison -- but those are no fun anyway.The scenario: two applications, using Massive.js to store and retrieve data. Massive is closely coupled to Postgres by design. Specializing lets it take advantage of features which only exist in some or no other relational databases to streamline data access in a lighter, more ""JavaScripty"" way than a more traditional object-relational mapper. It's great for getting things done, since the basics are easy and for the complicated stuff where you'd be writing SQL anyway.... you write SQL, you store it in one central place for reuse, and the API makes running it simple.Where Massive is less useful is if you have to support another RDBMS. This is, ideally, something you know about up front. Anyway: things happen, and sometimes you find yourself having to answer the question ""what's it going to look like if we need to run these applications with light but tightly coupled data layers on MySQL?""Not good, was the obvious answer, but less immediately obvious was how not good. I knew there were some things Postgres did that MySQL didn't, but I also knew there were a ton of things I'd just never tried in the latter. So as I got to work on this, I started keeping notes. Here's everything I found.Now that we're all basically over the collective hallucination of a ""schemaless"" future, arguably the most important aspect of data storage is how information is modeled in a database. Postgres and MySQL are both relational databases, grouping records in strictly-defined tables. But there's a lot of room for variation within that theme.First things first: ""schema"" doesn't always mean the same thing. To MySQL, ""schema"" is synonymous with ""database"". For Postgres, a ""schema"" is a namespace within a database, which allows you to group tables, views, and functions together without having to break them apart into different databases.MySQL's simplicity in this respect is ameliorated by its offering cross-database queries:With Postgres, you can work across schemas, but if you need to query information in a different database, that's a job for...Foreign data wrappers let Postgres talk to practically anything that represents information as discrete records. You can create a ""foreign table"" in a Postgres database and SELECT or JOIN it like any other table -- only under the hood, it's actually reading a CSV, talking to another DBMS, or even querying a REST API. It's a powerful enough feature that NoSQL stalwart MongoDB sneakily built their BI Connector on top of Postgres with foreign data wrappers. You don't even need to know C to write a new FDW when Multicorn lets you do it in Python!Oracle and SQL Server both have some functionality for registering external data sources, but Postgres' offering is the most extensible I'm aware of. MySQL, besides the inter-database query support mentioned above, has nothing.Inheritance is more commonly thought of as an attribute of object-oriented programming languages rather than databases, but Postgres is technically an ORDBMS or object-relational database management system. So you can have a table cities with columns name and population, and a table capitals which inherits the definition of cities but adds an of_country column only relevant, of course, for capital cities. If you SELECT from cities, you get rows from capitals -- they're cities too! You can of course SELECT name FROM ONLY cities to exclude the capitals. This is something of a niche feature, but when you have the right use case it really shines.MySQL, being a traditional RDBMS, doesn't do this.Materialized views are like regular views, except the results of the specifying query are physically stored ('materialized') and must be explicitly refreshed. This allows database developers to cache the results of slower queries when the results don't have to be realtime.Oracle has materialized views, and SQL Server's indexed views are similar, but MySQL has no materialized view support.Constraints in general ensure that invalid data is not stored. The most common constraint is NOT NULL, which prevents records without a value for the non-nullable column from being inserted or updated. Foreign key constraints do likewise when a reference to a record in another table is invalid. Check constraints are the most flexible, and allow validation of any predicate you could put in a WHERE clause -- for example, asserting that prices have to be positive numbers, or that US zip codes have to be five digits.Per the MySQL docs: the CHECK clause is parsed but ignored by all storage engines.Postgres and MySQL both have a JSON column type (MySQL replacement MariaDB does too, but it's currently just an alias for LONGTEXT) and functions for building, processing, and querying JSON fields. Postgres actually goes a step further by offering a JSONB type which processes input data into a binary format. This means it's a little bit slower to write, but much faster to query.It also means you can index the binary data. A GIN or Generalized INverted index allows queries checking for the existence of specific keys or key-value pairs to avoid scanning every single record for matches. This is huge if you run queries which dig into JSON fields in the WHERE clause.DEFAULT is a useful specification for columns in a CREATE TABLE statement. At the simplest level, this could be used to baseline a boolean field to true or false if the INSERT statement doesn't give an explicit value. But you can do more than set a scalar value: a timestamp can default to now(), a UUID to any of a variety of UUID-generating functions, any other field to the value returned by whatever function you care to write -- the sky's the limit!Unless you're using MySQL, in which case the only function you can reference in a DEFAULT clause is now().Layout's only part of the story, though. Equally important is the difference in type support. The benefit of a robust type system is in enabling database architects to represent information with the greatest accuracy possible. If a value is difficult or impossible to represent with built-in types, it's harder for developers to work with in turn, and if compromises have to be made to cut the data to fit then they can affect entire applications. Some types can even affect the overall database design, such as arrays and enumerations. In general, the more options you have the better.Postgres has a UUID type. MySQL does not. If you want to store a UUID in MySQL, your options are CHAR, if you want values to be as human-readable as UUIDs ever are, or BINARY, if you want it to be faster but more difficult to work with manually. Postgres also generates more types of UUIDs.Boolean seems like a pretty basic type to have! However, MySQL's boolean is actualy an alias for TINYINT(1). This is why query results show 0 or 1 instead of true or false. It's also why you can set the value of an ostensibly boolean field to 2. Try it!Postgres: has proper booleans.MySQL isn't alone in aliasing standard types in strange ways, however. CHAR, VARCHAR, and TEXT types in Postgres are all aliased representations of the same structure -- the only distinction is that length constraints will be enforced if specified. The documentation notes that this is actually slower, and recommends that unbounded text simply be defined as the TEXT type instead of given an arbitrary maximum length.What's happening here is that Postgres uses a data structure called a varlena, or VAriable LENgth Array, to store the information. A varlena's first four bytes store the length of the value, making it easy for the database to pick the whole thing out of storage. TEXT is only one of the types that uses this structure, but it's easily the most commonly encountered.If a varlena is longer than would fit inline, the database uses a system called TOAST (""The Oversized Attribute Storage Technique"") to offload it to extended storage transparently. Queries with predicates involving a TOASTable field might not be all that performant with large tables unless designed and indexed carefully, but when the database is returning records it's easy enough to follow the TOAST pointer that the overhead is barely noticeable for most cases.The upshot of all this, as far as most people are concerned, is this: with Postgres, you only have to worry about establishing a length constraint on fields that have a reason for a length constraint. If there's no clear requirement to limit how much information can go into a field, you don't have to pick an arbitrary number and try to match it up with your page size.Non-scalar values in records! Madness! Dogs and cats living together! Anyone who's worked with JSON, XML, YAML, or even HTML understands that information isn't always flat. Relational architectures have traditionally mandated breaking out any vectors, let alone even more complex values, into new tables. Sometimes that's useful, but often enough it adds complexity to no real purpose. Inlining arrays makes many tasks -- such as tagging records -- much easier.Postgres has arrays, as does Oracle; MySQL and SQL Server don't.If the built-in types aren't sufficient, you can always add your own. Custom types let you define a value to be exactly what you want. Domains are a related concept: types (custom or built-in) which enforce constraints on values. You might for example create a domain to represent a zip code as a TEXT value which uses regular expressions in a CHECK clause to ensure that values consist of five digits, optionally followed by a dash and four more digits.If you're using Postgres, that is. Oracle and SQL Server both offer some custom type functionality, but MySQL has nothing. You can't even use table-level CHECK constraints because the engine simply ignores them.Enumerations don't get enough love. If I had a dollar for every INT -- or worse, VARCHAR -- field I've seen representing one of a fixed set of potential values, I probably still couldn't retire but I could at least have a pretty nice evening out. There are drawbacks to using enums, to be sure: adding new values requires DDL, and you can't remove values at all. But appropriate use cases for them are still reasonably common.MySQL and Postgres both offer enums. The critical distinction is that Postgres' enums are proper reusable types. MySQL's enums are more like the otherwise-ignored CHECK constraints and specify a valid value list for a single column in a single table. Possible improvement on allowing a boolean column to contain -100?So that's data modeling covered. There's an entire other half to go: actually working with the information being stored. SQL itself is divided in two parts, the ""data definition language"" which defines the structure of a database and the ""data manipulation language"". This latter comprises the SELECT, INSERT, and other statements most people think of when they hear the name ""SQL"". And just as with modeling, there are substantial differences between Postgres and MySQL in querying.Autogenerating primary keys takes a huge headache out of storing data. But there's one catch: when you insert a new record into a table, you don't know what its primary key value got set to. Most relational databases will tell you what the last autogenerated key was if you call a special function; some, like SQL Server, even let you filter down to the single table you're interested in.Postgres goes above and beyond with the RETURNING clause. Any write statement -- INSERT, UPDATE, DELETE -- can end with a RETURNING [column-list], which acts as a SELECT on the affected records. RETURNING * gives you the entire recordset from whatever you just did, or you can restrict what you're interested in to certain columns.That means you can do this:With MySQL, you're stuck with calling LAST_INSERT_ID() after you add a new record. If you added multiple, LAST_INSERT_ID only gives you the earliest new id, leaving you to work out the rest yourself. And of course, this is only good for integer primary keys.MySQL also has no counterpart to this functionality for UPDATEs and DELETEs. Competitor MariaDB supports RETURNING on DELETE, but not on any other kind of statement.Common Table Expressions or CTEs allow complex queries to be broken up and assembled from self-contained parts. You might write this:In the first query, we aggregate visit counts; in the second, we use DISTINCT ON on the results of the first to filter out all but the most popular pages; finally, we join both of our intermediary results to provide the output we're looking for. CTEs are a really readable way to factor query logic out, and they let you do some things in one statement that you can't otherwise.MySQL does have CTEs! However: thanks to the RETURNING clause, Postgres can write records in a CTE and operate on the results. This is huge for application logic. This next query writes a record in a CTE, then adds a corresponding entry to a junction table -- all in the same transaction.Sometimes a query needs to treat a value as if it has a different type, whether to store it or to operate on it somehow. Postgres even lets you define additional conversions between types with CREATE CAST.MySQL supports casting values to binary, char/nchar, date/datetime/time, decimal, JSON, and signed and unsigned integers. Absent from this list: tinyints, which, since booleans are actually tinyints, means you're stuck with conditionals when you need to coerce a value to true or false for storage in a ""boolean"" column.A lateral join is fundamentally similar to a correlated subquery, in that it executes for each row of the current result set. However, a correlated subquery is limited to returning a single value for a SELECT list or WHERE clause; subqueries in the FROM clause run in isolation. A lateral join can refer back to information in the rest of the result set:It can also invoke table functions like unnest which return multiple rows and columns:Oracle and SQL Server offer similar functionality with the LATERAL keyword in the former, and CROSS APPLY/OUTER APPLY. MySQL does not.Functions! Procedures, if you believe in making that distinction! They're great! You can declare variadic arguments -- ""varargs"" or ""rest parameters"" in other languages -- to pull an arbitrary number of arguments into a single collection named for the final argument.In Postgres.A handful of useful operations which allow more expressive WHERE clauses with Postgres:That's it for the architecture and query language feature gaps I discovered. I did run into a couple other things that bear mentioning, however:MySQL doesn't care about dependencies among database objects. You can tell it to drop a table a view or proc depends on and it will go right ahead and drop it. You'll have no idea something's gone wrong until the next time you try to invoke the view or proc. Postgres saves you from yourself, unless you're really sure and drop your dependents too with CASCADE.Just the mention of triggers is probably putting some people off their lunch. They're not that bad, honest (well, they can be, but it's not like it's their fault). Anyway, point is: sometimes you want to write a trigger that modifies other rows in the table it's being activated from.Well, you can't in MySQL.This may have exhausted me, but I'm pretty sure it's still not an exhaustive list of the feature gaps between Postgres and MySQL. I did cop to my preference up front, but having spent six weeks putting the effort into converting the comparison is pretty damning. I think there could still be reasons to pick MySQL -- but I'm not sure they could be technical.",The Ultimate Postgres vs MySQL Blog Post,2018-04-11T17:48:14Z
https://dev.to/kiwicopple/loading-json-into-postgres-2l28,"Today I had to load some JSON data into Postgres.Postgres' COPY command it expects one JSON object per line rather than a full array.For example, instead of a JSON array:It needs to be this format:It took me a surprisingly long time to get the data into Postgres, but the solution was fairly simple.Here's how I did it.This is done with one command:ExplanationFrom here it's easiest to ingest the data into a JSONB column.20 seconds of reading, and 1 hour of my time. To get the data out of the table now you can use any of Postgres' amazing JSON support. For example:Enjoy.",Loading JSON into Postgres,2020-04-16T14:00:03Z
https://dev.to/condenastitaly/when-food-meets-ai-the-smart-recipe-project-b3g,"Did you ever try a Maritozzo?In the past post, we converted the recipe data, stored in JSON files, into RDF triples. In this post, we show you:To query the graph, we use SPARQL. SPARQL is an RDF query language, namely a semantic query language for databases, able to retrieve and manipulate data stored or viewed in the RDF format.We followed the described procedure to load the RDF triples on the Amazon Neptune service. We used an Amazon Simple Storage Service, the Amazon S3 bucket. Firstly we created an S3 bucket; then we uploaded the data. In this first phase, we loaded the RDF data to build the first level of the graph (see the previous article).In the case we want to add a few recipes at the time, we can alternatively use the SPARQL statement INSERT DATA :Once the recipes have been loaded, we checked whether there are recipes not yet processed by the extractor and classifier services. This means to check which recipes have not i) food entity chunks extracted (the bnode in the graph, see the previous article); ii) ingredients classified.This is the SPARQL query to check whether bnodes exist in the graph (through the statement FILTER NOT EXISTS), which is equivalent to say “return all the recipes without bnodes”:Now the graph is on Amazon Neptune. Let’s have fun of these connections, extracting knowledge from the graph:With the above query we interrogate the graph to know 1) whether there are recipes containing the ingredient “butter” and 2) which are these recipes. The WHERE statement navigates the graph following the pattern described in the triples to arrive at the query result. In this case, the output is the id of the recipes which have the ingredients ”butter”. We can query the graph to return recipes containing more than one ingredient or all the recipes containing some ingredients and not others:With this last article, we conclude illustrating the main stages of the Smart Recipe Project, this innovative and amazing project involving on one side the global company Condé Nast, and on the other the IT company RES.We have in mind some possible interesting applications for the resources we developed under the Smart Recipe Project like:personalization of contents, personalized recipe searchers, newsletter; recommendation systems for food items, recipes, and menus, which integrate, where needed, dietary restrictions; virtual assistants, able to guide you in planning and cooking meals; smart cooking devices, and much more.As always, go on Medium to read the complete article.When Food meets AI: the Smart Recipe Project a series of 6 amazing articlesTable of contentsPart 1: Cleaning and manipulating food data Part 1: A smart method for tagging your datasets Part 2: NER for all tastes: extracting information from cooking recipes Part 2: Neither fish nor fowl? Classify it with the Smart Ingredient Classifier Part 3: FoodGraph: a graph database to connect recipes and food data Part 3. FoodGraph: Loading data and Querying the graph with SPARQL",When Food meets AI: the Smart Recipe Project,2020-08-07T10:05:49Z
https://dev.to/adamcowley/building-a-modern-web-application-with-neo4j-and-nestjs-38ih,"This article is the introduction to a series of articles and a Twitch stream on the Neo4j Twitch channel where I build an application on top of Neo4j with NestJS and a yet-to-be-decided Front End. This week I built a Module and Service for interacting with Neo4j.TL;DR: I've pushed the code to Github and created a Neo4j module for NestJS to save you some time.Over the past few weeks I have been spending an hour live streaming something that I have found interesting that week, but from this week I thought I would change things up and start to build out a project on Neo4j.If you're subscribed to this channel then you are likely familiar with Neo4j, but if not then Neo4j is the world's leading Graph Database. Rather than tables or documents, Neo4j stores it's data in Nodes - those nodes are categorised by labels and contain properties as key/value pairs. Those Nodes are connected together by relationships, which are categorised by a type and can also contain properties as key/value pairs.What sets Neo4j apart from other databases is it's ability to query connected datasets. Where traditional databases build up joins between records at read time, Neo4j stores the dataNeo4j is schema-optional - meaning that you can enforce a schema on your database if necessary by adding unique or exists constraints on Nodes and Relationships.I've been experimenting with Typescript for a while now, and the more I use it the more I like it.Typescript is essentially Javascript but with additional static typing. Under the hood, it compiles down to plain Javascript but it improves the developer experience a lot, and allows you to identify problems in real-time as you are writing your code.By far the best framework I have seen that supports typescript is NestJS. NestJS is an opinionated framework for building server-side applications. It also includes modern features you'd expect in a modern framework like Spring Boot or Laravel - mainly Dependency Injection.Nest comes with a CLI with many helpers for starting and developing a project. You can install it by running:Once it's installed, you can use the new or n command to create a new project.After selecting the package manager of your choice, the CLI command will generate a new project and install any dependencies. Once it's done, you can cd into the directory and then run npm run start:dev to fire up the development server.In the generated src/ folder, you'll see:Functionality in Nest is grouped into modules, the official documentation uses Cats as it's example. Modules are a way of grouping related functionality together. In the Cats example, the module provides a CatsService which handles the applications interactions with Cats, and a Cats controller which registers routes which define how the Cats are accessed.Module classes are defined by a @Module annotation, which in turn defines which child modules are imported into module, any controllers that are defined in the module, and any classes that are exported from the module and made available for dependency injection.Take the annotation on the Cats example in the documentation, this is saying that the CatsModule registers a single controller CatsController and provides the CatsService.The CatService is registered with the Nest instance and can then be injected into any class.Classes annoted with @Injectable() are automatically injected into a class using some under-the-hood Nest ""magic"". For example, by defining the CatsService in the constructor for the CatsController, Nest will automatically resolve this dependency and inject it to the class without any additional code.This is identical to how things work in more mature frameworks like Spring and Laravel.Dependency Injection is a software technique where a class will be ""injected"" with instances of other classes that it depends on. This makes the testing process easier where instead of instantiating classes. It also promotes the principles of DRY - don't repeat yourself and SOLID. Each class should have a single responsibility - for example a User service should only be concerned with acting on a User's record, not be concerned with how that record is persisted to a database.In order to use Neo4j in services across the application, we can define a Neo4jService for interacting with the graph through the JavaScript driver. This service should provide the ability to interact with Neo4j but without the service itself needing to know any of the internals. This service should be wrapped in a module which can be registered in the application.The first step is to install the Neo4j Driver.Then, we can use CLI to generate a new module with the name Neo4j.The command will create a neo4j/ folder with it's own module. Next, we can use the CLI to generate the Service:This command will generate neo4j.service.ts and append it to the providers array in the module so it can be injected into any application that uses the module.By default, these modules are registerd as static modules. In order to add configuration to the driver, we'll have add a static method which accepts the user's Neo4j credentials and returns a DynamicModule.The first thing to do is generate an interface that will define the details allowed when instantiating the module.The driver takes a connection string and an authentication method. I like to split up the connection string into parts, this way we can validate the scheme.The scheme (or protocol) at the start of the URI should be a string, and one of the following options:The host should be a string, port should either be a number or a string, then username, password should be a string. The database should be an optional string, if the driver connects to a 3.x version of Neo4j then this isn't a valid option and if none is supplied then the driver will connect to the default database (as defined in neo4j.conf - dbms.default_database).Next, for the static method which registers the dynamic module. The documentation recommends using the naming convention of forRoot or register. The function should return a DynamicModule - this is basically an object that contains metadata about the module.The module property should return the Type of the module - in this case Neo4jModule. This module will provide the Neo4jService so we can add the class to the provides array.Because we are providing a configuration object, we'll need to register it as a provider so that it can be injected into the Neo4jService. For providers that are not defined globally, we can define a unique reference to the provider and assign it to a variable. We will use this later on when injecting the config into the service. The useValue property instructs Nest to use the config value provided as the first argument.If the user supplies incorrect credentials, we don't want the application to start. We can create an instance of the Driver and verify the connectivity using an Asynchronous provider. An async provider is basically a function that given a set of configuration parameters, returns an instance of the module that is configured at runtime.In a new file neo4j.utils.ts, create an async function to create an instance of the driver and call the verifyConnectivity() to verify that the connection has been successful. If this function throws an Error, the application will not start.The function accepts the Neo4jConfig object as the only argument. Because this has already been defined as a provider, we can define it in the injects array when defining it as a provider.Now that the driver has been defined, it can be injected into any class in it's own right by using the @Inject() annotation. But in this case, we will add some useful methods to the Neo4jService to make it easier to read from and write to Neo4j. Because we have defined NEO4J_DRIVER in the provides array for the dynamic module, we can pass the NEO4J_DRIVER as a single parameter to the @Inject directive in the constructor.Each Cypher query run against Neo4j takes place through a Session, so it makes sense to expose this as an option from the service. The default access mode of the session allows the Driver to route the query to the right member of a Causal Cluster - this can be either READ or WRITE. There is also an optional parameter for the database when using multi-tenancy in Neo4j 4.0. As I mentioned earlier, if none is supplied then the query is run against the default database.So the user doesn't need to worry about the specifics of read or write transactions, we should create a method for each mode - both with an optional parameter for the database. There is also a database specified in the Neo4jConfig object, so we should fall back to this if none is explicitly specified.These methods make use of NEO4J_CONFIG and NEO4J_DRIVER which were injected into the constructor.So with that in mind, it would be useful to create a method to read data from Neo4j. The driver accepts parameterised queries as a string (eg. queries with literal variables replaced with parameters - $myParam) and an object of parameters so these will be the arguments for the query. Optionally, we may want to specify which database this query is run against so it makes sense to include that as an optional third parameter. The query then returns a Result statement which includes the result and some additional statistics.Over the course of the application, this will save us a few lines of code. The same can be done for a write query:Now we have a service that is registered in the main application through the Neo4jModule that can be injected into any class in the application. So as an example, let's modify the Controller that was generated in the initial command. By default, the route at '/' returns a hello world message, but instead let's use it to return the number of Nodes in the database.To do this, we should first inject the Neo4jService into the controller:Now, we can modify the getHello method to return a string. The constructor will automatically assign the Neo4jService to the class so it is accessible through this.neo4jService. From there we can use the .read() method that we've just created to execute a query against the database.Navigating in the browser to http://localhost:3000 should now show a message including the number of nodes in the database.Tune in to the Neo4j Twitch channel Tuesdays at 13:00BST, 14:00CEST for the next episode.",Building a Modern Web Application with Neo4j and NestJS,2020-07-02T08:52:46Z
https://dev.to/codaelux/running-dynamodb-offline-4k1b,"I am assuming you are already familiar with the Serverless Framework so I'll jump right into itAmazon DynamoDB:- is a fully managed proprietary NoSQL database service that supports key-value and document data structures designed to deliver fast and predictable performance.The Serverless Framework:- enables developers to deploy backend applications as independent functions that will be deployed to AWS Lambda.let's kick off by configuring AWS if not already configured doing this you will have to download the AWS CLI for Windows, macOS, or LinuxThe serverless framework will need us to configure access to AWS - This is accomplished by runningInstall Serverless globally if you haven't installed it alreadyNow let's setup up the serverless-offline pluginInstall serverless-dynamodb-local pluginUpdate your serverless.yml file - add this to the bottom of your serverless.yml fileInitialize DynamoDB - this initializes DynamoDB offline in your project folderRun serverless:-Be sure DynamoDB is running on port 8000 because serverless-offline will use that port as defaultyou should see the output belowKeep this console running.Now let's setup dynamodb-admin: this gives us a good GUI to view our DynamoDB tablesNow runWe should have our table displayedConfiguring serverless.yml for DynamoDBI published a great cheat sheet with all commands needed for setting up DynamoDB offline you can find it herehttps://github.com/codaelux/DynamoDB-offline-doc",How to run DynamoDB Offline,2020-03-23T21:48:31Z
https://dev.to/arthurolga/newsql-an-implementation-with-google-spanner-2a86,"What is actually NewSQL? You’ve probably already dealt with SQL and NoSQL databases. Each with its own advantages and disadvantages. In this article, I’m going to present the main characteristics of NewSQL databases and a simple implementation using Google Spanner.Firstly, a little recap about the main topics concerning relational e and no-relational databases.SQL is known for being sturdy and organized, dealing with a set of properties called ACID (Atomicity, Consistency, Isolation, Durability), which is the main reason it is so popular, bringing a lot of these sturdiness. But this also means that it only scales vertically, so large companies might need a single powerful machine to support all their requests.NoSQL, on the other hand, is very flexible, without the need for the relational structure of the regular SQL, being able to support non-structured forms of data. This type of database is known for being able to scale horizontally, which means that companies can build database servers close the global clients. But, these non-relational databases don’t support ACID transactions, which means they cannot provide the same consistency as the regular relational.NewSQL comes with the idea of bringing the major advantages in each SQL and NoSQL to the same service. These databases use various techniques to provide these functionalities, as such:Partitioning/Sharding: In order to be able to scale horizontally, NewSQL uses a system of dividing itself in various shards/nodes/partitions. Differently form the fragmentation of the NoSQL databases, the shards run partial parts of the whole databases, although the whole works as a distributed application.Main Memory Storage: This characteristic provides the ability for the database to run on the memory, instead of a hard disc or flash memory. This provides the NewSQL a lot of leverage in terms of speed.Replication and Consistency: NewSQL replicates itself through transparent nodes using the Paxos or Raft consensus algorithm. They are families of complex protocols able to assure consensus in a network of unreliable processes.Concurrency Control: The NewSQL applies Multi-Version Concurrency Control (MVCC), Timestamp methods and T/O to assure the access is granted to the necessary nodes given a certain operation. This together with the consensus algorithm provides the ability of the database to support ACID transactions.This all makes NewSQL databases capable of OLTP, unlike any other databases, considering even SQL supports it partially.One of the best ways to implement a NewSQL database is using Google's DBaaS solution, called Spanner. Which can start a instance ready in minutes.But this all comes with a cost, a large cost. A simple Google Spanner Database with two nodes can cost about 2.00 USD per hour, a lot more than regular services. The main reason this service costs that much is because of the in-memory implementation, which is a lot pricier than a normal hard disc. Also, as the transparent shards work with redundancy, the space needed to store data can sometimes be very large to provide the consistency needed.We are going to build a Google Spanner Instance running with two nodes. Be careful as this will result in charges of around 2.00 USD per hour, so you can delete it after this tutorial. For this, we're going to need Python 2.7 and a Google Cloud account.Supposing you already have a Google Account with a credit card. Enter in your Google Cloud console, specifically at the Projects page: https://console.cloud.google.com/cloud-resource-managerClick on the ""Create Project"" button. We are going to create a new project for this tutorial, but you can also use one of your own.Choose a Project Name that suits you, but you are going to have to remember it for the later parts. Click on ""Create"" and wait for your project to be ready.Now, activate de Google Spanner API on your project: https://console.cloud.google.com/flows/enableapi?apiid=spanner.googleapis.comWe are going to deploy the instance using the CLI. You can also do this using the Google Cloud Console on the web and switch to the command line at any moment.Open your terminal and check if you have Python 2.7 installed and on your path:You can install it using Homebrew with:Alternatively you can install using the interactive installer bellow.Run:Reboot the shell:Use the GUI installer for windows, available at: https://cloud.google.com/sdk/downloads#windowsRun on the terminal:Chrome will open a window for you to login with your Google Account.Run to show all available projects:The project you just created should appear like this:Now you want to select this project as default. If you choose not to, you will have to pass a project parameter at every command.Now we are going to run our instances with two nodes. I'm choosing us-east1 as the region because it is the cheapest by the time I'm writing this tutorial.To see the instance you have created, run:Which should return as such:The instance is identified by test-instance, and the display name is My Instance.Now we are going to build a database and a table. As I've mentioned earlier, we can straight up use SQL language to work with this Database, so we're going to build some simples queries.To create a database called classroom:We are going to build a simples student table. For this, we are going to run a DDL command with the content of the query being:To run this on the Google CLI, we are going to run a command on the directory of this file with:You can use gcloud spanner databases ddl update DATABASE every time you want to alter the database schema.Now, to add run a DML we are going to use execute-sql:To add some more rows:Now we can see the table with:Which should returnNow to delete this expensive test instance:As you've seen, running commands on the CLI can be useful, but to build a real service, like a REST application, you need to be able to run these commands in any sort of language. So we're going to run a query using Python 3.7,** **but you can choose from a variety of different languages, such as C#, NodeJS or Ruby.Now we are going to run a query using a program. First You should have Python3 and pip installed.Run:Now create a file name **test-spanner.py **with the inicialization of the Client and Instance and the run of a simple Select query:Running this with python3 test-spanner.py should print on the terminal something like:This concludes this Tutorial, entirely based on the Google Spanner Documentation. Which is awesome! If you want to continue using this database, you should really check it out. Thanks for checking this out!Google Spanner Documentation: https://cloud.google.com/spanner/docs/",NewSQL: An Implementation with Google Spanner,2020-08-10T16:01:25Z
https://dev.to/javinpaul/5-best-courses-to-learn-apache-kafka-in-2020-584h,"Disclosure: This post includes affiliate links; I may receive compensation if you purchase products or services from the different links provided in this article.You might have heard about Apache Kafka, the next generation big data messaging system which is handling billions of messages per day for companies like LinkedIn, Uber, Airbnb, and Twitter.In the past, I have shared some awesome courses on Big Data, Spark, and Hadoop and many of my readers requested that I share similar suggestions for Apache Kafka. If you are interested in Apache Kafka and looking for some awesome courses to learn online then you have come to the right place.In this article, I am going to share some of the best Apache Kafka courses to learn online. If you know about big data then there is a good chance that you have also heard about Apache Kafka, the software which can handle data feeds for any organization. It's a distributed streaming platform developed by the Apache Foundation for building real-time data pipelines.The biggest advantage of Apache Kafka is its speed and horizontal scalability, which means you can increase capacity and throughput by adding more hardware. This makes it ideal for companies dealing with big data.Apache Kafka was designed to address large-scale data movement problems and has enabled hundreds of companies to achieve successes which were not otherwise possible with existing messaging systems.You might not know that Apache Kafka is written in Scala and Java and it aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds.This is a great course to start learning Apache Kafka from Scratch. In this course, instructor Ryan Plant will teach you everything about the architecture of Apache Kafka which enabled it to scale and perform so reliably.After the initial overview, the course moves to explain individual components of Apache Kafka, like Kafka Streams for processing real-time data feeds and you how to develop Apache Kafka solutions in Java.After this course, you should have all the necessary knowledge to build your own, next-generation big data solutions with Apache Kafka.Here is the link to join the course --- Getting Started with Apache KafkaBy the way, you need a Pluralsight membership to access this course, which cost around $29 per month. But, if you want, you can get access to this course for a fee by signing up for a 10-day free trial.This is another good course to learn Apache Kafka from ground zero. It's an ideal course for both developers and architects who want to learn the fundamentals of Apache Kafka.In this course, instructor Stephane Maarek, author of a series of Apache Kafka courses on Udemy will teach you everything about the Apache Kafka ecosystem from its architecture and core concepts to operations.The course is also hands-on as you will start a personal Kafka Cluster for development purposes and create and configure topics for reading and writing data.You will also learn to integrate Apache Kafta with popular programming and big data frameworks like Spark, Akka, Scala, and Apache NiFi.Here is the link to Sign up ---Learn Apache Kafka for BeginnersThis is another awesome course on Apache Kafka by Stephane Maarek. This course is focused on Kafka Streams, a client-side library for building microservices, where input and output data are stored in a Kafka cluster.In this course, you will learn how to use the Kafka Streams API with hands-on examples in Java 8. Though, before attempting this course you should have a good understanding of both Java 8 and Apache Kafka.You will also learn about KStream and KTable, simple and advanced operations, and Exactly Once Semantics, or EOS, like how Kafka enables EOS and how to activate it in Kafka Streams.Here is the link to Sign up --- Apache Kafka Streams for Data ProcessingThis course is part of the Big Data Hadoop Architect master's program in SimpliLearn and it will teach you everything about Apache Kafka you want to know. It's a certification course so it covers a variety of topics.In this Apache Kafka training course, you will learn about Kafka architecture, installation, interfaces, and configuration.The course starts with an overview of big data and then explains ZooKeeper and Apache Kafka from the introduction to installation.Here is the link to Sign up --- Apache Kafka Certification TrainingThis is the third course in the Apache Kafka series by Stephane Marek on Udemy. In this course, you will learn about Kafka Cluster Setup and Administration.You will set up a ZooKeeper and Kafka cluster on AWS and learn how to deploy Kafka in production. You will also set up a ZooKeeper Cluster and understand its role in Kafka.This is an ideal course for system Administrators or Architects who want to learn how to set up a Kafka Cluster on multiple serversHere is the link to Sign up --- Apache Kafka Cluster Setup and AdministrationThat's all about some of the best courses to learn Apache Kafka for Java developers. Apache Kafka is a groundbreaking technology and power more than 2000+ companies for their high speed messaging need and a good knowledge of Apache Kafka will go a long way to boost your career. I strongly recommend experienced Java developer, tech lead and solution architect to learn and understand Apache Kafka.Other Programming Resources you may like:Thanks for reading this course so far. If you like these Apache Kafka online training courses and certification then please share with your friends and colleagues. If you have any questions or feedback then please drop a note.",5 Best courses to learn Apache Kafka for Java Programmers,2019-12-21T04:50:27Z
https://dev.to/subhransu/realtime-chat-app-using-kafka-springboot-reactjs-and-websockets-lc,"In this tutorial, we would be building a simple real-time chat application that demonstrates how to use Kafka as a message broker along with Java, SpringBoot as Backend, and ReactJS on the front-end.This project is just for learning purposes. It doesn't contain a production-ready code.Apache Kafka is a widely popular distributed messaging system that provides a fast, distributed, highly scalable, highly available, publish-subscribe messaging system.In turn, this solves part of a much harder problem:Communication and integration between components of large software systems.Before starting the project, We need to download Zookeeper and Kafka.You can download Kafka from here.Extract the contents of the compressed file into a folder of your preference. Inside the Kafka directory, go to the bin folder. Here you’ll find many bash scripts that will be useful for running a Kafka application.If you are using Windows, you also have the same scripts inside the windows folder. This tutorial uses Linux commands, but you just need to use the equivalent Windows version if you’re running a Microsoft OS.Zookeeper is basically to manage the Kafka cluster. It comes bundled with the downloaded Kafka directory. So, we need not download it separately.To start the zookeeper, go to the bin directory and enter the below command.Next, To start the Kafka broker, run the below command in the same directoryMake sure the zookeeper is running before starting Kafka because Kafka receives information such as Offset information kept in the partitions from Zookeeper.After running Zookeeper and Apache Kafka respectively, We can create a Topic and send and receive data as Producer and Consumer.Here we are creating a topic kafka-chat to handle chat messages. We would be using this topic later in the chat application.Now, Let's write some code.We would be developing the backend in Spring Boot. So, download a fresh Spring Boot Project using Spring Initializer with the following details.Since Apache Kafka cannot send the Consumer Messages instantly to the client with Classical GET and POST operations. I performed these operations using WebSockets which provide full-duplex bidirectional communication, which means that information can flow from the client to the server and also in the opposite direction simultaneously. It is widely used in chat applications.First lets create a Message Modal which would hold the message content. Message.javaFirst, we would have to write a Config class for the Producer.ProducerConfiguration.javaThis class creates a ProducerFactory which knows how to create producers based on the configurations we provided.We also declared a KafkaTemplate bean to perform high-level operations on your producer. In other words, the template can do operations such as sending a message to a topic and efficiently hides under-the-hood details from you.In producerConfigurations method, we need to perform the following tasks:The next step is to create an endpoint to send the messages to the Kafka topic. Create the following controller class for that.As you can see the endpoint is quite simple. When we do POST request to /api/send it Injects the KafkaTemplate configured earlier and sends a message to the kafka-chat topic which we created earlier.Let's test everything we build until now. Run the main method inside KafakaJavaApp.java class. To run from the command line, execute the following commandYour server should be running on port 8080 and you can make API requests against it! You can use postman to do a POST request as shown below.But how do you know the command successfully sent a message to the topic? Right now, you don’t consume messages inside your app, which means you cannot be sure!Fortunately, there is an easy way to create a consumer to test right away. Inside the bin folder of your Kafka directory, run the following command:Hit http://localhost:8080/api/send again to see the message in the terminal running the Kafka consumerNow let's achieve the same functionality using the Java Code. For that, we would need to build a Consumer or Listener in Java.Similar to ProducerConfig.java we need to have a Consumer Config to enable the consumer to find the broker.ListenerConfig.javaIn Consumer Config, similar to Producer Config we are setting the deserializer for key and value. Additionally we need to setMessageListener.javaIn this class, the @KafkaListener annotated the method that will listen for the Kafka queue messages, and template.convertAndSend will convert the message and send that to WebSocket topic.Next, we need to configure the Websocket to send the Message to the client system.WebSocketConfig.javaNext add the below MessageMapping in the ChatController.javaThis would broadcast the Message all the client who have subscribed to this topic.Next, let's move on to developing the UI part.We would create a simple chat page with a list of messages and a text field at the bottom of the page to send the messages to Kafka backend.We will use Create React App to quickstart the app.Install dependenciesYou can refer documentation of material-ui here.Copy the CSS styleCopy the css style from here paste it in the App.css file.Next, add the below changes to App.jsApp.jsHere we are using SocketJsCLient from react-stomp to connect to the WebSocket.Alternatively, you can also use SockJS from sockjs-client to create a stompclient and connect to the WebSocket.Next, we need to create Messages Child Component which would show the list of messages.LoginForm.jsOpen the application in multiple windows and send a message in one window. All the other browser window should show the sent messages.we are using SockJS to listen to the messages, which are sent from the server-side WebSocket.You can find the complete source code in my Github page.","Realtime Chat app using Kafka, SpringBoot, ReactJS, and WebSockets",2020-04-25T23:17:22Z
https://dev.to/offlineprogrammer/collect-analytics-data-for-your-app-using-aws-amplify-4ifp,"Wouldn’t be great to get some insights on the behavior of your App users. This will allow you implement data driven features to drive customer adoption, engagement, and retention.This is where AWS Amplify can help, in this post I will show you how you can integrate your Android App with AWS Amplify to achieve this. Let’s get started.Check the full code on github and follow me on Twitter for more tips about #coding, #learning, #technology, #Java, #JavaScript, #Autism, #Parenting...etc.Check my Apps on Google Play",Collect analytics data for your App using AWS Amplify,2020-08-09T21:35:31Z
https://dev.to/guthakiran/building-a-cluster-using-elasticsearch-kibana-zookeeper-kafka-and-rsyslog-17ja,"Here in this tutorial you will learn about how to build a cluster by using elasticsearch, kibana, zookeeper, kafka and rsyslog. Developers can face many difficult situation when building a cluster, here we clearly explained step by step procedure to create a cluster.Environmental difficulties: 　　1. The developer cannot log in to the server 　　2. Each system has a log, the log data is scattered and difficult to find 　　3 large amount of log data, the query is busy, can not be real-timeEnvironmental requirements: 　　1. The log needs to be standardizedRequired software for building this cluster: logstash-2.0.0.tar.gz Elasticsearch-2.1.0.tar.gz Jdk-8u25-x64.rpm Kafka_2.11-0.11.0.1.tgz Kibana-4.3.2-linux-x64.tar.gzThe above software can be downloaded from the official website: https://www.elastic.co/downloadsClose the firewall and close selinux (production environment is turned off or on as needed) Synchronize server time, select public network ntpd server or self-built ntpd server[root@es1 ~]# crontab -l # To facilitate the direct use of the public network server*/5 * * * * /usr/bin/rdate -s time-b.nist.gov &>/dev/nullThe elk runtime needs the jvm environment. The 2.x version needs the oracale JDK 1.7 or open-jdk1.7.0 version. The 5.X version requires the oracale JDK 1.8 or the open-jdk1.8.0 version. The multi-node JDK version ensures that it always contains minor versions. No., otherwise it may be an error when joining the cluster, which is why the younger brother did not use yum to install the JDK.[root@es1 ~]# rpm -ivh jdk-8u25- x64.rpm # Because 5.X version requires 1.8, in order to upgrade later to install 1.8 directly Preparing... ########################################### [100%] 1:jdk1.8.0_131 ########################################### [100%][root@es1 ~]# cat /etc/profile.d/ java.sh #Edit the Java environment configuration file export JAVA_HOME=/usr/java/latest export CLASSPATH=$JAVA_HOME/lib/tools.jar export PATH=$JAVA_HOME/bin:$PATH [root@es1 ~]# . /etc/profile.d/java.sh [root@es1 ~]# java - version #Confirm Configuration java version ""1.8.0_131"" Java(TM) SE Runtime Environment (build 1.8.0_131-b11) Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode)[root@es1 ~]# tar xf elasticsearch-2.1.0.tar.gz -C /usr/local/ [root@es1 ~]# cd /usr/local/ [root@es1 local]# ln -sv elasticsearch-2.1.0 elasticsearch ""elasticsearch"" -> ""elasticsearch-2.1.0"" [root@es1 local]# cd elasticsearch [root @ es1 elasticsearch] # vim config / elasticsearch.yml [root@es1 elasticsearch]# grep ""^[a-Z]"" config/elasticsearch.yml Cluster.name: pwb - cluster #Cluster name, which must be configured in the same cluster Node.name: pwb - node1 # Cluster node name, unique within the cluster Path.data: /Data/es/ data #data directory Path.logs: /Data/es/ logs #log directory bootstrap.mlockall: true network.host: 10.1.1.243 http.port: 9200 discovery.zen.ping.unicast.hosts: [""10.1.1.243"", ""10.1.1.244""] discovery.zen.minimum_master_nodes: 1[root@es1 elasticsearch]# mkdir -pv /Data/es/{data,logs} Mkdir: created directory "" /Data "" mkdir: created directory "" /Data/es "" mkdir: created directory "" /Data/es/data "" mkdir: created directory "" /Data/es/logsCreate a normal user and suggest adding the appropriate sudo permissions [root@es1 elasticsearch]# useradd elasticsearch [root@es1 elasticsearch]# chown -R elasticsearch:elasticsearch /Data/es/ [root@es1 elasticsearch]# chown -R elasticsearch:elasticsearch /usr/local/elasticsearch-2.1.0/[root@es1 elasticsearch]# echo ""elasticsearch hard nofile 65536"" >> /etc/security/limits.conf [root@es1 elasticsearch]# echo ""elasticsearch soft nofile 65536"" >> /etc/security/limits.conf [root@es1 elasticsearch]# sed -i 's/1024/2048/g' /etc/security/limits.d/90-nproc.conf [root@es1 elasticsearch]# echo ""vm.max_map_count=262144 "" >> /etc/sysctl.conf [root@es1 elasticsearch]# sysctl -p[root@es1 elasticsearch]# grep "" ES_HEAP_SIZE= "" "" bin/ elasticsearch # Set elasticsearch memory size, in principle, the bigger the better, but do not exceed 32G Export ES_HEAP_SIZE =100m # The test environment has limited memoryThe configuration of other nodes is the same as that of the other nodes.Network.host: 10.1 . 1.243 # Local IP AddressNode.name: pwb -node1 # The assigned node name[root@es1 elasticsearch]# su - elasticsearch [elasticsearch@es1 ~]$ cd /usr/local/elasticsearch [elasticsearch@es1 elasticsearch]$ bin/elasticsearch&Through the output you can see the service startup and add other nodes in the cluster through auto discovery.Check if the service is normal[root@es1 ~]# netstat -tnlp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 944/sshd Tcp 0 0 ::ffff: 10.1 . 1.243 : 9300 :::* LISTEN 3722 / java Communication between # ES nodes tcp 0 0 :::22 :::* LISTEN 944/sshd Tcp 0 0 ::ffff: 10.1 . 1.243 : 9200 :::* LISTEN 3722 / java # ES node and external communication use [root@es1 ~]# curl http: // 10.1.1.243:9200/ # If the following message appears, the installation and configuration is successful { "" name "" : "" pwb-node1 "" , "" cluster_name "" : "" pwb-cluster "" , "" version "" : { "" number "" : "" 2.1.0 "" , "" build_hash "" : "" 72cd1f1a3eee09505e036106146dc1949dc5dc87 "" , ""build_timestamp"" : ""2015-11-18T22:40:03Z"", ""build_snapshot"" : false, ""lucene_version"" : ""5.3.1"" }, ""tagline"" : ""You Know, for Search"" }[root@es1 ~]# /usr/local/elasticsearch/bin/plugin install mobz/elasticsearch-headAfter the installation is complete, visit the URL http://10.1.1.243:9200/_plugin/head/. Since there is no data in the cluster for the moment, the display is empty (the five-pointed star indicates the master node, and the dot indicates the data node).Other commonly used plug-in installation methods (not demonstrated here, are interested in their own installation)./bin/plugin install lukas-vlcek/bigdesk # 2 .0 version change commands . /bin/plugin install hlstudio/bigdesk # 2 .0 or later Use this command to install , / bin / plugin install lmenezes / elasticsearch-head / versionLogstash needs to rely on the java environment, so here we still need to install the JVM, this step is omitted[root@logstash1 ~]# tar xf logstash-2.0.0.tar.gz -C /usr/local/ [root@logstash1 ~]# cd /usr/local/ [root@logstash1 local]# ln -sv logstash-2.0.0 logstash ""logstash"" -> ""logstash-2.0.0"" [root@logstash1 local]# cd logstash [root@logstash1 logstash]# grep ""LS_HEAP_SIZE"" bin/logstash.lib.sh LS_HEAP_SIZE = "" ${LS_HEAP_SIZE:=100m} "" # Set memory size to use(1) Write a logstash configuration file[root@logstash1 logstash]# cat conf/messages.conf input { File { # data input using input file plugin, read from messages file path => ""/var/log/messages"" } } output { Elasticsearch { # data output points to ES cluster Hosts => [ "" 10.1.1.243:9200 "" , "" 10.1.1.244:9200 "" ] # ES Node Host IP and Port } } [root@logstash1 logstash]# /usr/local/logstash/bin/logstash -f conf/messages.conf --configtest --verbose Configuration OK [root@logstash1 logstash]# /usr/local/logstash/bin/logstash -f conf/messages.conf Default settings used: Filter workers: 1 Logstash startup completed(2) Write some files to message, we install some software[root@logstash1 log]# yum install httpd -yCheck the changes in the messages file[root@logstash1 log]# tail /var/log/messages Oct 24 13:44:25 localhost kernel: ata2.00: configured for UDMA/33 Oct 24 13:44:25 localhost kernel: ata2: EH complete Oct 24 13:49:34 localhost rz[3229]: [root] logstash-2.0.0.tar.gz/ZMODEM: error: zgethdr returned 16 Oct 24 13:49:34 localhost rz[3229]: [root] logstash-2.0.0.tar.gz/ZMODEM: error Oct 24 13:49:34 localhost rz[3229]: [root] no.name/ZMODEM: got error Nov 8 22:21:25 localhost rz[3245]: [root] logstash-2.0.0.tar.gz/ZMODEM: 80604867 Bytes, 2501800 BPS Nov 8 22:24:27 localhost rz[3248]: [root] jdk-8u25-x64.rpm/ZMODEM: 169983496 Bytes, 1830344 BPS Nov 8 22:50:49 localhost yum[3697]: Installed: apr-util-ldap-1.3.9-3.el6_0.1.x86_64 Nov 8 22:50:50 localhost yum[3697]: Installed: httpd-tools-2.2.15-60.el6.centos.6.x86_64 Nov 8 22:51:07 localhost yum[3697]: Installed: httpd-2.2.15-60.el6.centos.6.x86_64Visit web page for elasticsearch head plugin. It has been seen that logstash can be normally written to the elasticsearch cluster and the logstash configuration is completed (the other nodes are configured the same).When building a kafka cluster, you need to install the zookeeper cluster in advance. You can install it separately or you can use the kafka to install the zookeeper program. Choose the zookeeper program that comes with kafka.[root@kafka1 ~]# tar xf kafka_2.11-0.11.0.1.tgz -C /usr/local/ [root@kafka1 ~]# cd /usr/local/ [root@kafka1 local]# ln -sv kafka_2.11-0.11.0.1 kafka ""kafka"" -> ""kafka_2.11-0.11.0.1"" [root@kafka1 local]# cd kafka[root@kafka1 kafka]# grep ""^[a-Z]"" config/zookeeper.properties dataDir=/Data/zookeeper clientPort=2181 maxClientCnxns=0 tickTime=2000 initLimit=20 syncLimit=10 server.1=10.1.1.247:2888:3888 server.2=10.1.1.248:2888:3888tickTime: This time is used as the interval between heartbeats maintained by ZooKeeper servers or between the client and the server. That is, a heartbeat is sent every tickTime.Port 2888: indicates the port where this server exchanges information with the leader server in the cluster.port: indicates that if the leader server in the cluster is hung up, a port is required to re-election and a new leader is selected. This port is the port through which the servers communicate with each other during the election.[root@kafka1 kafka]# mkdir -pv /Data/zookeeper Mkdir: created directory "" /Data "" mkdir: created directory "" /Data/zookeeper "" [root@kafka1 kafka]# echo "" 1 "" > /Data/zookeeper/myid # myid file, the contents of which are numbers for Identify the host, if this file does not, zookeeper can not start[root@kafka1 kafka]# grep ""^[a-Z]"" config/server.properties broker.id = 1 # unique in the Listeners = PLAINTEXT: // 10.1.1.247:9092 # server IP address and port num.network.threads = 3 num.io.threads = 8 socket.send.buffer.bytes = 102400 Socket.Receive .buffer.bytes = 102400 socket.request.max.bytes = 104857600 log.dirs = / the Data / kafka- logs need to create in advance # Num.partitions = 10 # need to be configured larger, sharding affects write and read speeds num.recovery.threads.per.data.dir = 1 offsets.topic.replication.factor = 1 transaction.state.log.replication.factor = 1 Transaction.state.log.min.isr = 1 log.retention.hours = 168 # expiration time log.segment.bytes = 1073741824 log.retention.check.interval.ms = 300000 zookeeper.connect = 10.1 . 1.247 : 2181 , 10.1 . 1.248 : 2181 # zookeeper server IP and port zookeeper.connection.timeout.ms=6000 group.initial.rebalance.delay.ms=0The other nodes have the same configuration except the following:( 1 ) Zookeeper configuration echo "" x "" > /Data/zookeeper/ myid # Unique ( 2 ) Configuration of Kafka Broker.id =1 # unique host.name = local IP[root@kafka1 kafka]# /usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties &The following two stations perform the same operation. During the startup process, the following error message appearsjava.net.ConnectException: Connection refused at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339) at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200) at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) at java.net.Socket.connect(Socket.java:579) at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:562) at org.apache.zookeeper.server.quorum.QuorumCnxManager.toSend(QuorumCnxManager.java:538) at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.process(FastLeaderElection.java:452) at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.run(FastLeaderElection.java:433) at java.lang.Thread.run(Thread.java:745) [2018-06-05 23:44:36,351] INFO Resolved hostname: 10.1.1.248 to address: /10.1.1.248 (org.apache.zookeeper.server.quorum.QuorumPeer) [2018-06-05 23:44:36,490] WARN Cannot open channel to 2 at election address /10.1.1.248:3888 (org.apache.zookeeper.server.quorum.QuorumCnxManager) java.net.ConnectException: Connection refused at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339) at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200) at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) at java.net.Socket.connect(Socket.java:579) at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:562) at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:614) at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:843) at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:913)Since the zookeeper cluster is started, each node tries to connect to other nodes in the cluster. The first startup is certainly not connected to the following nodes. Therefore, the exceptions in the previous section of the log are negligible. As you can see from the latter part, the cluster is finally stable after selecting a leader. Other nodes may also have similar conditions, which are normal[rootkafka1 ~]# netstat -nlpt | grep -E ""2181|2888|3888"" tcp 0 0 :::2181 :::* LISTEN 33644/java tcp 0 0 ::ffff:10.1.1.247:3888 :::* LISTEN 33644/java[root@kafka2 ~]# netstat -nlpt | grep -E ""2181|2888|3888"" tcp 0 0 :::2181 :::* LISTEN 35016/java Tcp 0 0 ::ffff: 10.1 . 1.248 : 2888 :::* LISTEN 35016 / java # which is the leader, then he has 2888 ports tcp 0 0 ::ffff:10.1.1.248:3888 :::* LISTEN 35016/java/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties &If you have the following error[2018-06-05 23:52:30,323] ERROR Processor got uncaught exception. (kafka.network.Processor) java.lang.ExceptionInInitializerError at kafka.network.RequestChannel$Request.(RequestChannel.scala:124) at kafka.network.Processor$$anonfun$processCompletedReceives$1.apply(SocketServer.scala:518) at kafka.network.Processor$$anonfun$processCompletedReceives$1.apply(SocketServer.scala:511) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at kafka.network.Processor.processCompletedReceives(SocketServer.scala:511) at kafka.network.Processor.run(SocketServer.scala:436) at java.lang.Thread.run(Thread.java:745) Caused by: java.net.UnknownHostException: kafka2.example.com: kafka2.example.com: Unknown name or service at java.net.InetAddress.getLocalHost(InetAddress.java:1473) at kafka.network.RequestChannel$.(RequestChannel.scala:43) at kafka.network.RequestChannel$.(RequestChannel.scala) ... 10 more Caused by: java.net.UnknownHostException: kafka2.example.com: Unknown name or service at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method) at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901) at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1293) at java.net.InetAddress.getLocalHost(InetAddress.java:1469) ... 12 moreEdit the hosts file and add 127.0.0.1 to resolve the current host name [root@kafka1 ~]# cat /etc/hosts 127.0.0.1 kafka1.example.com localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6Start zookeeper and kafka on other nodes After the startup is complete, perform some tests[root@kafka1 ~]# /usr/local/kafka/bin/kafka-topics.sh --create --zookeeper localhost: 2181 --replication-factor 2 --partitions 1 -- topic summer # NOTE :factor size cannot More than the number of brokers, otherwise an error occurs. The current cluster broker value is 2 Created topic ""summer"".[root @ kafka1 ~] /usr/local/kafka/bin/kafka-topics.sh --list --zookeeper 10.1 . 1.247 : 2181 # cluster list all Topic Summer[root@kafka1 ~]# /usr/local/kafka/bin/kafka-topics.sh --describe --zookeeper 10.1.1.247:2181 --topic summer Topic:summer PartitionCount:1 ReplicationFactor:2 Configs: Topic: summer Partition: 0 Leader: 2 Replicas: 2,1 Isr: 2,1The # Replicas copy exists on top of the broker id 2, 1 .[rootkafka1 ~]# /bin/bash /usr/local/kafka/bin/kafka-console-producer.sh --broker-list 10.1 . 1.247 : 9092 -- topic summerHello,MR.John #Enter something, Enter[root@kafka2 kafka]# /usr/local/kafka/bin/kafka-console-consumer.sh --zookeeper 10.1.1.247:2181 --topic summer --from-beginning Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of zookeeper. Hello,MR.JohnIf you can receive the message from the producer as above, then the kafka-based zookeeper cluster is successful.[root@log-client1 ~]# rsyslogd -v rsyslogd 5.8.10, compiled with:Rsyslog support for kafka is provided after v8.7.0 release, all need to upgrade the system rsyslog versionWget http://rpms.adiscon.com/v8-stable/rsyslog.repo -O /etc/yum.repos.d/rsyslog.repo # Download the yum source yum　update rsyslog -y 　　 Yum install rsyslog -kafka -y # install rsyslog- kafka module Ll /lib64/rsyslog/ omkafka.so # Check whether the module is installed /etc/init.d/rsyslog restart # Restart the serviceCheck for updated version[root@log-client1 yum.repos.d]# wrsyslogd -v rsyslogd 8.30.0, compiled with:(1) Edit the rsyslog configuration file[root@log-client1 yum.repos.d]# cat /etc/rsyslog.d/nginx_kafka.confmodule(load=""omkafka"") module(load=""imfile"")template(name=""nginxAccessTemplate"" type=""string"" string=""%hostname%<-+>%syslogtag%<-+>%msg%\n"")ruleset(name=""nginx-kafka"") { # Log forwarding kafka action ( type=""omkafka"" template=""nginxAccessTemplate"" confParam=[""compression.codec=snappy"", ""queue.buffering.max.messages=400000""] partitions.number=""4"" topic=""test_nginx"" broker=[""10.1.1.247:9092"",""10.1.1.248:9092""] queue.spoolDirectory=""/tmp"" queue.filename=""test_nginx_kafka"" queue.size=""360000"" queue.maxdiskspace=""2G"" queue.highwatermark=""216000"" queue.discardmark=""350000"" queue.type=""LinkedList"" queue.dequeuebatchsize=""4096"" queue.timeoutenqueue=""0"" queue.maxfilesize=""10M"" queue.saveonshutdown=""on"" queue.workerThreads=""4"" ) }input(type=""imfile"" Tag=""nginx,aws"" File=""/var/log/nginx/access.log"" Ruleset=""nginx-kafka"")Test conf file for syntax error[root@log-client1 yum.repos.d]# rsyslogd -N 1 rsyslogd: version 8.30.0, config validation run (level 1), master config /etc/rsyslog.conf rsyslogd: End of config validation run. Bye.Restart rsyslog after the test is complete. Otherwise, the configuration does not take effect.[root@log-client1 yum.repos.d]# /etc/init.d/rsyslog restartStart nginx, add two test pages, accessSwitch to kafka cluster server and check the topic list[root@localhost ~]# /usr/local/kafka/bin/kafka-topics.sh --list --zookeeper 10.1.1.247:2181 summer Test_nginxYou can see that in addition to the summer created by the previous test, an additional test_nginx topic is configured.[root@logstash1 ~]# cat /usr/local/logstash/conf/test_nginx.conf input { kafka{ zk_connect => ""10.1.1.247:2181,10.1.1.248:2181"" # kafka group_id => ""logstash"" topic_id => ""test_nginx"" reset_beginning => false consumer_threads => 5 decorate_events => true } } output { elasticsearch { hosts => [""10.1.1.243:9200"",""10.1.1.244:9200""] # elasticsearch index => ""test-nginx-%{+YYYY-MM}"" } }Test grammar[root@logstash1 ~]# /usr/local/logstash/bin/logstash -f /usr/local/logstash/conf/test_nginx.conf -t Configuration OKStart the service, the remaining nodes also start the service[root@logstash1 log]# /usr/local/logstash/bin/logstash -f /usr/local/logstash/conf/test_nginx.conf Default settings used: Filter workers: 1 Logstash startup completedSwitch to the ES cluster node es1.example.com to see:[root@es1 ~]# curl -XGET '10.1.1.243:9200/_cat/indices?v&pretty' health status index pri rep docs.count docs.deleted store.size pri.store.size green open logstash-2017.11.08 5 1 4 0 40.7kb 20.3kb green open test-nginx-2017-11 5 1 1 0 12.5kb 6.2kbAs you can see, the test-nginx index already has. In the web interface to access the head plugin, retry the test, use forced refresh. Visit the head plugin web interface, the latest visit to two records has come out.Install kibana on ES cluster nodes[root@es1 ~]# tar xf kibana-4.2.1-linux-x64.tar.gz -C /usr/local/ [root@es1 ~]# cd /usr/local/ [root@es1 local]# ln -sv kibana-4.2.1-linux-x64/ kibana ""kibana"" -> ""kibana-4.2.1-linux-x64/"" [root@es1 local]# cd kibana [root@es1 kibana]# grep "" ^[aZ] "" config/ kibana.yml # Configure the host port to have the elasticsearch server IP address and port server.port: 5601 server.host: ""0.0.0.0"" elasticsearch.url: ""http://10.1.1.243:9200""Now let’s open the kibana, configure an index and create a view to test the data.Conclusion: That’s all for now, here we learned about how to build a cluster using ELK, Zookeeper, Kafka and Rsyslog. Let’s make your hands dirty, if you have any questions regarding this post please drop your comment in the below comment section. Happy learning.Author Bio: Kiran GuthaThe author has an experience of more than 6 years of corporate experience in various technology platforms such as Big Data, AWS, Data Science, Machine Learning, Linux, Python, SQL, JAVA, Oracle, Digital Marketing etc. He is a technology nerd and loves contributing to various open platforms through blogging. He is currently in association with a leading professional training provider, Mindmajix Technologies INC. and strives to provide knowledge to aspirants and professionals through personal blogs, research, and innovative ideas.","Building a Cluster Using Elasticsearch, Kibana, Zookeeper, Kafka and Rsyslog",2018-06-25T12:09:02Z
https://dev.to/presto412/hyperledger-fabric-transitioning-from-development-to-production-4dch,"You’ve set up your development environment. Designed your chaincode. Set up the client. Made some decent looking UI as well. Everything works fine locally. All of your tests pass, and all the services are up and running.All this, and you still aren’t satisfied. You want to do more. You want to scale the network. Simulate a production level environment. Maybe you even want to deploy to production. You give it an attempt. Add multiple orderers. Add more organizations with their own peers and CAs. You try testing it with multiple machines.This is where everything you’ve done fails, no matter what you try. You somehow debug everything and come up with some nifty hacks you’re not entirely proud of. Some parts work, some don’t. You wind up the day, still not satisfied.I don’t know if you can relate to this. I went through this phase, racking my head and taking multiple cups of coffee trying to clear my head and starting the debugging from scratch. Multiple facepalms and “ah, this is how this works” later, I decided to write this article, and be your friendly neighbourhood developer-man(sorry).Fabric provides two consensus protocols for the network - Solo, and Kafka-Zookeeper. If you’ve only been working with Solo mode(configurable in configtx.yaml), this is where you change it. When we want our network to be shared among more than 3 peers, it makes sense to have multiple orderers, in the case where one particular, cursed node goes down. Now by design, orderers are like the postmen in a Fabric network. They fetch and relay the transactions to the peers. Solo mode requires all our orderers to be up, and if one goes down the entire network goes down. So here, the trick is to use a Kafka based ordering service. I’ve comprehensively explained how it works in Fabric here.Fabric docs provide some great best practices to follow.So here goes our first step - Solo to Kafka. Don’t forget to specify the brokers in your transaction config files. A sample has been provided below.configtx.yamlNow that we have our orderers’ dependencies resolved, let’s dive into some network level tips.All the images that we use for Hyperledger Fabric are docker images, and the services that we deploy are dockerized. To deploy to production, we’ve two choices. Well, we have a lot of choices, but I’ll only describe the ones that might just not get you fired. For now.The two choices, are Kubernetes and Docker Swarm. I decided to stick with Docker Swarm because I didn’t really like the hacky docker-in-docker set up for the former. More on this here.From the Docker docs,“A swarm consists of multiple Docker hosts which run in swarm mode and act as managers (to manage membership and delegation) and workers (which run swarm services). A given Docker host can be a manager, a worker, or perform both roles”.It is a cluster management and orchestration method, that is featured in Docker Engine 1.12.So consider this - the blockchain administrator can have access to these manager nodes, and each potential organization can be a designated worker node. This way, a worker node will only have access to the allowed resources, and nothing gets in the way. Swarm uses the raft consensus algorithm, and the managers and workers accomodate replication of services, thus providing crash tolerance.Not giving hostnames to your services is a rookie mistake. A hostname for every service is necessary. This is how one service in the Swarm maps the IP address to the host name. This is used extensively for Zookeeper ensembles.Consider 3 Zookeeper instances. We would have these depend on each other for synchronization. When these are deployed to a swarm, the Swarm gives an internal IP address, which is pretty dynamic.Another Zookeeper instance that would depend on this,Usually, inside a Swarm, the services are distributed to any node by default. Docker Swarm will always try to improve the performance on the manager nodes. Therefore, it will try to distribute the services to the worker nodes. When a worker node goes down, the manager can redistribute the services inside the swarm.Now there are some very important services that we would like to keep afloat, like the Kafka-Zookeeper ensemble, since they synchronize transactions among all the orderers. Hence, what we would like to do here is make sure that we don’t suffer any downtime. There also may be a service that holds certificates, and it is important that the certificates don’t leak in the Swarm network. Therefore, we need restrictions on stack deployment in the Swarm.We can constrain the services deployed to nodes in the swarm. A simple example is shown below.Note that the deploy section defines the crash handling as well, so use it to your benefit. When we implement constraints, almost every authentication issue can be resolved. You might observe that this defeats the purpose of docker Swarm, where it is supposed to maintain the state of the Swarm as much as possible. If such is the case, you’ll have to spend more on another node that can handle downtime and probably extend your constraints.Certificates are not supposed to be disclosed to other entities in a network. Ideally, the easiest thing to do, is to supply the certificates of a particular organization to the node hosting it, and install to a location that is common to both a docker container and a simple linux machine, like /var/<network-name>/certs. This way, you mount only the required volumes. Speaking of volumes, when mounting, be sure to have absolute paths. You can only deploy services to a Docker Swarm from a manager node, and hence it needs to have the certificates at the location in a node hosting it, else the service will shut down.An example:docker-compose-peer.ymlThe /var/network/certs/ directory should be copied in the host worker node before deploying the service.I don’t recommend a standalone service floating in the swarm, that anyone can access.Naming the servicesdocker stack deploy doesn’t allow certain characters for service creation. If you worked directly from the tutorials, you would have the service names like peer0.org1.example.com, orderer0.example.com.So it is better to name them as peer0_org1 , and orderer0 and so on.Docker Swarm usually prefixes the stack name to the service, and the suffix is a SHA256 hash. Hence, to execute any commands, we need the name of the services, given to them by the Swarm. So for example, if you’ve named your service peer0_org1, and the stack you’ve deployed it to is deadpool, the name that swarm will give it would look like deadpool_peer0_org1.1.sa213adsdaa…..You can fetch its name by a simple command,PRO TIP: Environment variables are your best friends. Do have a dedicated .env for all your scripts.When you have multiple organizations, and you want custom channels to run amongst these, and install different smart contracts on each channel, this is how it should work.So if you’re able to invoke and query successfully, you should be good to go.Document your configuration well, create some scripts that save time, like ssh-ing into the services and executing the above commands from a manager node itself.Attach your client to the services above, and enjoy your Production-level Fabric Network set up. Try deploying your network to the IBM cloud, or AWS.If you have any queries or suggestions, do comment below.",Hyperledger Fabric: Transitioning from Development to Production,2018-06-15T15:25:53Z
https://dev.to/goaty92/designing-tinyurl-it-s-more-complicated-than-you-think-2a48,"Recently I came across a Youtube video called: System Design : Design a service like TinyUrl, from the channel Tushar Roy - Coding Made Simple. This video discusses a common developer interview question, namely, how do you design a service like TinyURL, which allows users to turn long URLs into short ones that are just several characters long. Basically, a TinyURL-like service would have 2 main APIs: createShort(longUrl) and getLong(shortUrl). The second one is easy, you simply need to do a lookup and return the long URL (or 404 if none exists). The main problem is the createShort() API: How do you generate a short sequence of characters that is unique among URLs (note that uniqueness is an important property, we don't want different URLs to have the same shortcut).Tushar's proposed solutions are quite good and I think most interviewers would be satisfied with them (please watch the video before continuing to read this post). That being said, they are sort of unsatisfying. To summarize, the most sophisticated solution proposed in the video is to partition all possible short sequences into ranges, and use a set of servers to return a monotonically increasing sequence, which falls within a range. Each server would be in assigned only one particular range to work with, and Apache Zookeeper is used to coordinate the sequence range assignments. If the each server has a unique range, then they are guaranteed to generate unique sequences. The reason I think this answer is unsatisfying is because, while it works, it simply shifts the responsibility of generating the ""unique"" part of the sequence, which, is the hardest part of the problem, to Zookeeper. Instead of answering the question ""how to generate a unique sequence?"" (or sequence range, in this case), this solution simply says ""I'll just ask Zookeeper to give me one"". But how does Zookeeper do that?First of all, why is it so hard to generate a unique sequence? Afterall, I can use a single computer to keep increasing a counter, and that would be unique, right? In fact, that solution is mentioned by Tushar in the video, but later rejected, because the counter-generating server might fail (either the machine itself crashes, or the network might go down etc.), and Zookeeper, somehow, magically provides ""high availability"" (i.e. it is resilient to failures).And that's the gist of the problem. If I had the guarantee that my servers never fails, then I wouldn't need Zookeeper. I probably wouldn't need multiple servers either, one beefy machine might be enough to do the job. Unfortunately, in practice machines do fail, and in fact, they fail all the time. That is why when we design systems, we design for failure. In this case, when one servers in the Zookeeper cluster fails, somehow the system needs to make sure that the others don't return a duplicate range. The only way to do that is to make all servers agree on which ranges have been given, and which have not.So let's try to simplify & generalize the problem: given a set of servers, how do we ensure that all servers agree on a value, even if the servers might fail randomly (the value in this case would be the range assignment). This is know as the distributed consensus problem, which actually is one of the hardest problems in Distributed systems. In fact, it has been mathematically proven that, in an asynchronous system (meaning a system where we don't know how long it takes for messages to travel between servers), there is NO way to guarantee distributed consensus. This is known as the FLP Impossibility.Fortunately, in most of the systems in practice, we can workaround this issue by modelling them as ""partially synchronous systems"", that is, we can apply a boundary on how long it takes to send messages between servers. And in this model, consensus is possible. There are several algorithms that can be used to get consensus, like Paxos or Raft. Zookeeper itself uses a consensus protocol called Zab (which stands for Zookeeper atomic broadcast).I won't get into details on how these algorithms work. Afterall, they are quite complicated and sometimes difficult to understand. However if you ever need to work with those directly, an important thing to pay attention to is that they are not perfect. Raft and Paxos, for example, only works if the number of failed nodes is less than half the total number of nodes in the system. Failure also take different forms, and while Paxos and Raft works well with Fail-stop and Fail-safe types of failure, Byzantine-type failures are a lot harder to deal with.",Designing TinyURL: it's more complicated than you think,2020-08-10T10:21:05Z
https://dev.to/dihfahsih1/9-best-python-frameworks-for-building-small-to-enterprise-applications-2jla,"PYTHON IS BOTH A FUN TOY AND A FRIGHTENING FLAMETHROWER. SAME GOES WITH WHAT YOU CAN DO WITH PYTHON.Python is loved by hobbyists, scientists and architects alike.It’s damn easy to get started with, has higher-order abstractions and metaprogramming capabilities to build large and complex systems, and has truck-loads of libraries for doing pretty much anything. Sure, there are limitations when it comes to concurrency and strong typing, but you can work around them.In this article, we’ll cast a look at some of the best Python frameworks when it comes to building web applications large and small.And there’s a good reason for that. Django is, as the tagline says, “a web framework for perfectionists with deadlines.” It’s what is called a “batteries included” framework (much like how Python is a batteries-included language), which provides all common functionality out of the box.With these features baked in, Django massively cuts down on development time:A handy and pleasant ORM, with migrations created and applied automatically by the framework. Scaffolding for automatic generation of admin panel based on your models. Support for cookies, sessions, middleware, templates, etc. Security features like XSS prevention, CRSF prevention, etc., are applied automatically. Works with practically all databases out there (it’s easy to find adapters where official support doesn’t exist) First-class support for Geographical data and spatial queries though GeoDjango And much, much more. Suffice it is to say Django is a full-blown, friendly web framework.Is Django for you?Absolutely yes.Django makes excellent sense for all use cases, whether rapid prototyping or planning an enterprise application. The only rough edge you’ll come across is the framework’s structure. Since Django bends over backward to make development fast and easy for you, it imposes its structure (a concept called “convention over configuration”) on the developer, which you may not agree with. For instance, if you want to replace the Django ORM with something else (say, SQL Alchemy), be prepared for surprises.Interested in becoming full stack developer with Django and Python? – Check out this fantastic online course.As opposed to Django, Flask is a “micro-framework,” which means it focuses on getting a few, bare minimum things right, and leaves the rest to you. This “the rest is up to you” can be a source of frustration or delight, depending on what your goals are. For those that know what they’re doing and want to lovingly craft their web applications by choosing components of their choice, Flask is a godsend.Flask offers the following features:Routing, templating, session management, and other useful features. Full support for unit-testing A minimal, pluggable architecture First-class REST support Support for Blueprints, Flask’s unique take on architecture for tiny web applications Choose your packages for ORM, migrations, etc. Flexible application structure — put your files where they make the most sense to you Static file serving WGSI compliant Is Flask for you?As already said, Flask is a minimal web framework, with everything broken up into individual components that you can swap out. If you’re in a hurry to build a prototype, you’ll spend a lot of time making trivial decisions on the database, folder structure, routing, etc., that can prove counter-productive. Flask works best when you’re on to a stable, serious project of medium- to large-scale, especially REST APIs.Bottle strips out even more, to the point where the only dependency is the Python standard library. This means no pip install this or pip install that, though you’d most likely need to before long. Here’s why Bottle stands out for some people:Single-file deployment. Yes, your entire application lives in a single “.py” file. No external dependencies. If you have the right Python version installed, you’re good to go. Supplies its templating engine, which can be swapped out with Jinja2, Mako, or Cheetah. Support for forms, headers, cookies, and file uploads. Built-in web server, which can be easily replaced. Is Bottle for you?If you’re making a really small app (say, less than 500 lines of code) with no special requirements, Bottle might make a lot of sense to you. It’s a complete no-nonsense approach to creating web apps, but in practice, you’ll find you’re more hindered than helped by Bottle. The reason is that the real world is always changing and before you know it. New requirements will be dropped on your head. At that point, putting everything in a single file would become a chore.Also, if you think Bottle and Flask are almost alike, you’re right. Proposals of merging the two date back to 2012, and even Armin, the creator of Flask, agrees with that. However, Marcel, the creator of Bottle, maintains a strict ideological distinction because of the single-file approach and insists that the two remain separate.Zope has several interesting components and features suitable for enterprise application development:A component registering and discovery architecture to configure a large app. ZODB — (the only) object database for Python for storing objects natively. Full-fledged framework and standards for Content Management Systems A set of web application frameworks — the canonical one is still called Zope, although several new frameworks (like Grok) have been built on top of it. Strong standards for software development, release, and maintenance. Is Zope for you?If you’re after a highly structured environment for building really large apps, Zope is good. That said, you’ll run into your fair share of issues as well. While Zope continues to evolve, the community is really small, to the extent that many Python developers haven’t even heard of it. Finding tutorials and extensive documentation is hard, so be prepared to do a lot of digging around (though the community is really helpful!). Also, the Python developers you come across may not want to learn Zope and “dilute” their skill-set.TurboGears has some elegant features, some of which are either not present in popular frameworks (like Django) or are hard to build:First-class support for multiple databases Multi-database transactions Highly modular — start with a single file and scale out as much as you need A powerful ORM (SQLAlchemy, which is more mature and capable than Django’s ORM) Pluggable architecture based on the WSGI specification Built-in support for database sharding A function-driven interface as opposed to deep, rigid object-oriented hierarchies. Is TurboGears for you?If you want to develop happily and want a tested, mature, and robust framework away from the media noise of “awesome, next-gen” and all that, TurboGears is a great fit. It’s highly respected in the community and has complete, extensive documentation. Sure, TurboGears isn’t opinionated, which means initial setup and configuration time can be more, but it’s the ideal framework for enterprise application development.As a result, Web2py takes the zero-dependency approach to the extreme — it has no requirements, nothing to install, and includes a full-featured Web-based editor for development, database management, as well as deployment.You can almost think of it as Android Studio, which is more of a complete environment than just a framework. Some nice features that Web2py has, are:Virtually no learning curve. Minimal core (only 12 objects), which can even be memorized! Pure-Python templating Protection against XSS, CSRF, and other attacks A pleasant and consistent API Is Web2py for you?Web2py is a stable and fun framework, but it’s hard to recommend it against other options like Django, Flask, etc. There are hardly any jobs, and the unit testing story is not great. That said, you might enjoy the code API and the overall experience the framework offers, especially if you’re building REST APIs.While it’s comparable to other microframeworks like Flask, CherryPy boasts of some distinction:It contains a built-in multi-threaded server (something that remains on the wishlist of Flask) The (single) web server can host multiple applications! Serve your application as a WSGI app (to interface with other WSGI apps) or a plain HTTP server (which performs better) First-class support for profiling and unit-testing Runs on PyPy (for the true performance junkies), Jython, and even Android CherryPy does all this, and then the usual you’d expect from a web framework.Is CherryPy for you?If you’re building RESTful services mostly, CherryPy is a much more serious contender than Flask. It’s a decade-old framework that has matured nicely and is suitable for small and large applications alike.Sanic is heavily inspired by Flask, to the extent that it borrowed the route decorators, Blueprints, and other fundamentals hook line and sinker. And they’re not ashamed to admit it. What Sanic brings to the table, if you’re a Flask fan, is true non-blocking I/O to meet the performance levels of a Node application. In other words, Sanic is Flask with async/await support!When compared to CherryPy, Sanic has an incredible performance advantage (just think of how it would fare against Flask!). Check out the following results tested by DataWeave:As you can see, once the concurrency numbers start exceeding 50 per second, CherryPy practically chokes and throws up a high failure rate.Is Sanic for you?While the performance characteristics of Sanic blow everything else out of the water, it may not be the best choice for your next project. The main reason is the lack of asynchronous libraries. The bulk of existing Python tools and libraries were written for the single-threaded CPython version, with no forethought for high concurrency or asynchronous operations. If, for example, your favorite ORM does not support asynchronous operations, the whole point of using Sanic gets defeated.Because of these maturity and availability reasons, we won’t examine any more async frameworks in Python.9.Masonite I came across this framework a while ago and thought it was a step in the right direction. Since then, version 2.0 has been released, and I feel like the time has finally come to give Masonite some love.Simply put, Masonite is the Python version of Laravel (a famous PHP framework, in case you didn’t know). Why does that matter? It matters because Laravel was built on the principles of Ruby on Rails, and together these two frameworks allow non-Ruby devs to experience the “Rails Way” of doing things.Laravel (and to an extent, Rails) developers will feel right at home and would be up and running in literally no time. When I tried Masonite (and I did submit an issue or two, including a bug!), I was able to build REST APIs with exactly zero thinking because my Laravel muscle memory was doing everything.As a batteries-included, full-stack framework, Masonite brings several interesting things to the table:Active-record style ORM Database migrations (which, unlike Django, need to be created by the developer) A powerful IoC Container for dependency injection Own CLI (called “craft”) for scaffolding and running tasks First-class support for unit testing The biggest “rival” for Masonite is Django, as the community is doing its best to market the framework as easy, delightful, and the next big thing. Whether it will surpass Django is something time will tell (if you ask me, it does have a decent shot), but for a discussion comparing the two, see here and here.Is Masonite for you?Masonite is still a baby when compared to Django, so there’s no way it can be recommended over Django. That said, if you’re into the Rails way (or the Laravel way) of doing things, you’d appreciate what Masonite has to offer. It’s ideal for rapidly building prototypes that need everything pre-configured and easy to switch.Conclusion There’s no shortage of Python web frameworks out there, large and small. While you can pick up pretty much anything for a small project, an enterprise application has demands that not many of these frameworks can fulfill. If you ask me, for enterprise development, Django (to an extent), Zope, and TurboGears are what comes to mind. And even among those, I’m inclined towards TurboGears.That said, any architect worth his salt can pick up a microframework and roll out their architecture. And this is pretty much what happens in practice, which explains the success of Flask and similar ideas.If you are a newbie, then this online course would be helpful to learn Python.",9 Best Python Frameworks for Building Small to Enterprise Applications,2020-02-04T17:10:54Z
https://dev.to/sandipmavani/nodejs-vs-php-52g4,"There’s no doubt PHP is the most known and commonly used language for server-side scripting. Before Django and Ruby on Rails gained popularity (2005-2006), there was hardly any more suitable option for back-end than PHP. However, the tech world is fastly evolving in the direction of simplicity (“Javascript everywhere”) what used to be the language of the front-end has successfully expanded to the back-end. So now we are facing the popular back-end dilemma “Node.js vs PHP”. Let’s try to solve it together!PHP PHP (Hypertext Preprocessor) was created as a scripting language for back-end web development in 1994. For almost 10 years, it had been the only option for a back-end developer. Although lately, new technologies emerged, PHP wasn’t at a stop as well (the last stable version was released in January 2018). As of the latest statistics of 2018, more than 80% of websites are built with PHP (though some websites are built with more than one back-end language).Node.js Node.js is an open source run-time environment for back-end scripting in Javascript powered by Google’s V8 JS engine. It was created in 2009 and came up with the main advantage — Node.js allows to perform asynchronous programming. It can handle thousands of requests operating on one thread, which means no waiting until the previous command is completed. Although the percentage of websites that are built with Node.js is comparatively low (0,4%), it’s fastly becoming popular among developers.Node.js vs PHP: DifferencesLanguages Starting our Node.js vs PHP discussion, we can’t avoid talking about the specifics of two programming languages Javascript and PHP. In terms of syntax simplicity, PHP wins. Such an essential operation as an interaction with a database requires less and simpler (quite alike to HTML) code in PHP than in Node.js.However, the simplicity of PHP doesn’t come without certain restrictions. On the other hand, some complexity of Node.js is balanced by more possibilities of using various Javascript libraries for two-way asynchronous scripting of client and server.Also, Javascript syntax of Node.js is obviously a reason to choose it for those who come from the front-end side. So a full-stack developer may take an advantage from the code reusability options as a result of scripting both client and server sides in Javascript.Requirements Although PHP has a built-in web server (which is available since PHP 5), it’s not a full-featured one and can be used only for testing purposes. The most commonly used with PHP servers are Apache and NGINX. One need to get it installed and configure (by installing corresponding PHP components) a server before actually start working with PHP.Node.js doesn’t require any external servers. You can get started by installing Node.js. After that, you just need to install http-server packages via npm (which is just a few words of code) to use built-in web server tools. Also, installation of Express.js a Node web framework may help to do more than just trivial handling HTTP requests.When it comes to libraries, both PHP and Node.js have built tools that allow to install and manage them from the command line (Composer in PHP and npm in Node.js).Scalability Comparing Node.js vs PHP scalability, it turns out that both both technologies are scalable. It’s totally possible to build large scalable applications with either PHP or Node.js. Still there’s a difference that lies in the efficiency of building scalable application architecture.PHP is supported across most popular content management systems (such as Drupal, Joomla, WordPress), which makes it an often choice as a tool for building blogs and e-commerce web applications. In contrast, Node.js efficiently serves as a tool for creating scalable dynamic solutions that deal with numerous I/O operations. It’s also possible to scale Node on multi-cores systems, though with more efforts.Handling data When it comes to working with data, the main arm of Node.js is JSON, which is, on one hand, understandable by NoSQL and, on the other hand, is used for passing data on client side. And if early version of PHP had some troubles with JSON, it’s not an issue any more as JSON support is included in the core since PHP 5.2.However, the main advantage of PHP when it comes to working with data is that it’s compatible with all the hosting services. And the latter is very far from the reality for Node.js, which makes a good fit with only a narrow range of hosting services.Node.js vs PHP: SimilaritiesLanguages Both PHP and Javascript are interpreted languages that can be used not only for web programming but general purpose scripting. They require a certain run-time environment to be used for scripting (in the case of Javascript it may be either browser or server, which depends on whether it’s used for client- or server- side scripting).Performance Although Node.js is always highlighted as a high-performative one because of its asynchronous model, PHP has also evolved in this direction. With such libraries as ReactPHP, it has become possible for PHP to be used in event-driven programming as well.On the other hand, most servers function on multi-threading, which brings difficulties to work with Node.js. In such cases, asynchronous programming becomes more of an obstacle as not every intermediate-level programmer has enough expertise in it.Even though Node.js is faster (some Node.js vs PHP benchmarks can be found here), PHP 7 and Node.js are almost at the same level in terms of pure performance. In other words, handling asynchronous I/O operations isn’t something that can make Node.js a winner in Node.js vs PHP performance competition.Conclusion Both PHP and Node.js obviously have their own advantages and disadvantages. However, it doesn’t mean that you can’t build the same applications using either PHP or Node.js. So how can you pick one of them? I believe that the best way to make a choice of Node.js vs PHP 2018 is to pick the one that is most compatible with other technologies you use for developing web applications. It will save your time and resources.Apparently, if you are about to use most of MEAN (MongoDB, Express.js, AngularJS, Node.js) stack tools, numerous Javascript libraries (Angular, React, Backbone, etc.) or develop SPAs (Single Page Applications), Node.js may be a better fit for your project. On the other hand, if you take advantage of the most of LAMP (Linux, Apache, MySQL, PHP) stack technologies or other servers like SQL, Oracle, Postgresql you may need to consider PHP in the first place.Although discussions around Node.js vs PHP don’t seem to cease any soon, the important thing to remember is that there’s nothing unique that you can do only with one of them they are interchangeable. However, you can always orient at the level of development expertise and stack of technologies that are to be used in the process of development.",Node.js vs PHP,2018-09-24T06:12:38Z
https://dev.to/josiehall/import-data-from-s3-to-redshift-in-minutes-using-dataform-55g2,"Dataform is a powerful tool for managing data transformations in your warehouse. With Dataform you can automatically manage dependencies, schedule queries and easily adopt engineering best practices with built in version control. Currently Dataform integrates with Google BigQuery, Amazon Redshift, Snowflake and Azure Data Warehouse. However, often the “root” of your data is in another external source e.g. Amazon S3. If this is the case and you’re considering using a tool like Dataform to start building out your data stack, then there are some simple scripts you can run to import this data into your cloud warehouse using Dataform.We’re going to talk about how to import data from Amazon S3 to Amazon Redshift in just a few minutes, using the COPY command. This allows you to load data in parallel from multiple data sources. The COPY command can also be used to load files from other sources e.g. Amazon EMR or an Amazon DynamoDB table.An Amazon Web Services (AWS) account. Signing up is free - click here.Permissions in AWS Identity Access Management (IAM) that allow you to create policies, create roles, and attach policies to roles. This is required to grant Dataform access to your S3 bucket.Verified that column names in CSV files in S3 adhere to your destination’s length limit for column names. If a column name is longer than the destination’s character limit it will be rejected. In Redshift’s case the limit is 115 characters.An Amazon S3 bucket containing the CSV files that you want to import.A Redshift cluster. If you do not already have a cluster set up, see how to launch one here.A Dataform project set up which is connected to your Redshift warehouse. See how to do that here.Ok now you’ve got all that sorted, let’s get started! Once you’re in Dataform, create a new .sqlx file in your project under the definitions/ folder. Using Dataform’s enriched SQL this is what the code should look like:Finally, you can push your changes to GitHub and then publish your table to Redshift. Alternatively, you can run this using the Dataform CLI: dataform run.And Voila! Your S3 data is now ready to use in your Redshift warehouse as a table and can be included in your larger Dataform dependency graph. This means you can now run it alongside all other code, add dependencies on top of it (so any datasets that rely on this will only run if it is successful), you can use the ref() or resolve() functions on this dataset in another script and you can document it's data catalog entry using your own descriptions.For more information about how to get setup on Dataform please see our docs.",Import data from S3 to Redshift in minutes using Dataform,2019-10-10T15:52:10Z
https://dev.to/ahmetkucukoglu/couchbase-geosearch-with-asp-net-core-i04,"This article was originally published at: https://www.ahmetkucukoglu.com/en/couchbase-geosearch-with-asp-net-core/The subject of this article will be about how to do ""GeoSearch"" by using Couchbase.For this purpose, we create Couchbase cluster with the docker by running the command below.When Couchbase is up, it will start broadcasting at the address below.http://localhost:8091/We create the cluster by clicking the ""Setup New Cluster"" button. We define the password as ""123456"".You can make adjustment according to your current memory status. You can turn off ""Analytics"". We complete the cluster installation by clicking the ""Save & Finish"" button.We go to ""Buckets"" from the left menu, click the ""Add Bucket"" button at the top of the right corner and create a bucket called ""meetup"".http://localhost:8091/ui/index.html#!/bucketsAs an example, we will do the events section in the Meetup application that we all use. We will define events through the API. We will record the locations of these events. We will then query a list of events near by the current location.We create an ASP.NET Core API project. We install the nuget packages below.We add Couchbase connection information to the appsettings.json file.We define Couchbase in the Startup.cs file.In the project, we create a folder named ""Models"" and add a class named ""EventDocument"" into it. This class will be the model of the document that we will add to the bucket named ""events"" in Couchbase.Likewise, we add a class named ""CreateEventRequest"" under the ""Models"" folder. It will be the request model of endpoint, which will allow us to record this event.We add a controller named ""EventsController"".Now we can add events to Couchbase. Let's run the application and make requests from Postman as follows.When we look at the documents of events bucket in Couchbase, we will see that it has been added.http://localhost:8091/ui/index.html#!/doc_editor?bucket=eventsIn Couchbase, we come to ""Search"" from the left menu and click the ""Add Index"" button in the upper right corner.http://localhost:8091/ui/index.html#!/fts_new/?indexType=fulltext-index&sourceType=couchbaseWe write ""eventsgeoindex"" in the Name field. We select ""events"" from the Bucket field.Using the ""+ insert child field"" on the right side of the mapping named ""default"" under ""Type Mappings"", we add mappings as follows.We add mapping to save the ""Subject"" information in the event to ""eventsgeoindex"".We add mapping to save the ""Address"" information in the event to ""eventsgeoindex"".We add mapping to save the ""Date"" information in the event to ""eventsgeoindex"".We add mapping to search according to the ""Location"" information in the event.The final situation will be as in the image below.We create the index by clicking the ""Create Index"" button. When ""Indexing progress"" is 100%, it means that indexing has been finished.http://localhost:8091/ui/index.html#!/fts_list?open=eventsgeoindexWe add a class named ""GetUpcomingEventsRequest"" under the ""Models"" folder. This will be the request model of endpoint, which will return events near by our location.Likewise, we add a class named ""GetUpcomingEventsResponse"" under the ""Models"" file. This will be the endpoint's response model, which returns events that are near by our location.We add a controller named ""UpcomingEventsController"". In the line 30, we indicate that the search will be made on the basis of km. In the line 40, we indicate the name of the ""search index"" that we will create in Couchbase.Now we can list events near by our location. Let's run the application and make requests from Postman as follows. By typing 1 and 2 to ""Radius"", you can see the events near 1 or 2km.You can access the final version of the project from Github.Good luck.",Couchbase GeoSearch with ASP.NET Core,2020-05-20T04:26:53Z
https://dev.to/angular/outputting-json-ld-with-angular-universal-4ia1,"Originally published at https://samvloeberghs.be on March 12, 2019This article and guide on generating JSON-LD with Angular Universal is targeted to developers that want to generate the JSON that represents the Linked Data object and inject it correctly in the HTML that gets generated on the server. We will re-use and explain some techniques that the TransferState key-value cache service uses to transfer server-state to the application on the client side.JSON-LD is a lightweight Linked Data format. It is easy for humans and machines to read and write. It is based on the already successful JSON format and provides a way to help JSON data interoperate at Web-scale.Linked Data empowers people that publish and use information on the Web. It is a way to create a network of standards-based, machine-readable data across Web sites. It allows an application to start at one piece of Linked Data, and follow embedded links to other pieces of Linked Data that are hosted on different sites across the Web.These 2 definitions of JSON-LD and Linked Data were simply taken from the website of JSON-LD. This article does not go deep into the full specification of JSON-LD and it does not teach you how to properly generate your Linked Data structures. The tools of the JSON-ld website, like for example the playground, give you all you need to validate your Linked Data structures.What you can see below is a typical example of a JSON-LD data object, injected into the HTML. It describes a LocalBusiness by following its definition and providing the properties that identifiy the entity. For example, the address, it's geolocation, opening hours etc are given. As you can see the address has it's own type PostalAddress. Most of the types can specificy an external link and this is how data gets interweaved on the same website or resource and even on other websites.You should definitely care about JSON-LD for several reasons:In general; it ads a lot more semantic value to your HTML markup by providing context to what you show on the page and makes this context machine-readable, allowing it to be parsed more easily by search engines and other crawlers on the web.In the simple case of a blog or not too complex website, generating your JSON Linked Data can be done using the same base data you use to set your meta tags for social sharing and SEO. Your most important concern is building the correct structure. How your structure looks is completely dependent on your usecase. As a basic example we will use this website, and more specifically the about page.Using the router we first define the correct SEO data associated with our route. Just like we did before, setting the correct social share meta tags.Using a service we can subscribe to route changes to extract this data and pass the data to a service to parse the JSON-LD object.The JsonLdService injected above is a simple service that caches and updates the data structure we need to output. A basic implementation of this service can be as follows, where you as a developer are still in charge of correctly structuring your object.Using the approach explained above our current JSON-LD data-object in memory will look like this:This basic example is what we wanted to achieve. The next step is getting this object outputted in our static HTML.Injecting the JSON-LD data object in the DOM is only really required when we generate the HTML on the server. To get to this result I looked into the BrowserTransferStateModule(https://github.com/angular/angular/blob/master/packages/platform-browser/src/browser/transfer_state.ts) exposed via the @angular/platform-browser module.Essentialy this module, and more specifically, the TransferState(https://github.com/angular/angular/blob/master/packages/platform-browser/src/browser/transfer_state.ts) service it provides, does the same thing we want to achieve. It caches data, the result from for example HTTP calls, in an object and holds it in memory during the application lifecycle. Apart from the HTTP calls, that's exactly what our JsonLdService does.The server counter part, ServerTransferStateModule(https://github.com/angular/angular/blob/master/packages/platform-server/src/transfer_state.ts) exposed via the @angular/platform-server, serializes the data and injects it in the DOM before generating the HTML on the server and sending it back over the wire to the browser. Again, that is exactly what we want to achieve. This is achieved by providing an extra factory function to the BEFORE_APP_SERIALIZED token. Right after the application becomes stable all factories attached to the BEFORE_APP_SERIALIZED are executed.When the Angular application bootstraps in the browser, the TransferState service picks up the serialized state in the static HTML and unserializes it, making it available as a direct cache. This way we avoid that the application makes a similar request for the same data to the server, for which the server-side-rendered version already did the call. We don't need this part, but it's good to know.Following the concepts and ideas we learned from the ServerTransferStateModule and the BrowserTransferStateModule we create our own ServerJsonLdModule and BrowserJsonLdModule. The only small differences are limited to changing the type of the <script> element and injecting it in the <head> instead of right before the </body> tag. We also don't need to pickup any state from the static HTML, as is done in the BrowserTransferStateModule.To differentiate the execution of our application on the server versus on the browser, in Angular Universal we typically create a new server module app.server.module.ts next to app.module.ts that will directly import the AppModule exported by app.module.ts. This version of the application is used in the HTML renderer configured at the server side, in most cases as a rendering engine.If we go back to the example of the TransferState service, we see that the app.server.module.ts is importing the ServerTransferStateModule. This will overwrite the provider of the JsonLdService imported in the app.module.ts and provide the extra serialize functionality, next to also providing the TransferState service.We do the same thing with our BrowserJsonLdModule and ServerJsonLdModule. The browser module has to be imported in app.module.ts while the server module has to be imported in app.server.module.ts.Generating JSON-LD using Angular Universal is pretty straight-forward. By borrowing concepts from the Angular source code we can create our own JsonLd modules and services that enable us to output JSON-LD in the statically generated HTML on the server. The result of this code can be found live on this website, just look at the source and do a hard refresh. Why a hard refresh you might ask? Because of the service worker caching the default index.html as the app shell.Originally published at https://samvloeberghs.be on March 12, 2019",Outputting JSON-LD with Angular Universal,2019-06-07T18:39:16Z
https://dev.to/leonardomso/a-beginners-guide-to-graphql-3kjj,"One of the most commonly discussed terms today is the API. A lot of people don’t know exactly what an API is. Basically, API stands for Application Programming Interface. It is, as the name says, an interface with which people — developers, users, consumers — can interact with data.You can think of an API as a bartender. You ask the bartender for a drink, and they give you what you wanted. Simple. So why is that a problem?Since the start of the modern web, building APIs has not been as hard as it sounds. But learning and understanding APIs was. Developers form the majority of the people that will use your API to build something or just consume data. So your API should be as clean and as intuitive as possible. A well-designed API is very easy to use and learn. It’s also intuitive, a good point to keep in mind when you’re starting to design your API.We’ve been using REST to build APIs for a long time. Along with that comes some problems. When building an API using REST design, you’ll face some problems like:1) you’ll have a lot of endpoints2) it’ll be much harder for developers to learn and understand your API3) there is over- and under-fetching of informationTo solve these problems, Facebook created GraphQL. Today, I think GraphQL is the best way to build APIs. This article will tell you why you should start to learn it today.In this article, you’re going to learn how GraphQL works. I’m going to show you how to create a very well-designed, efficient, powerful API using GraphQL.You’ve probably already heard about GraphQL, as a lot of people and companies are using it. Since GraphQL is open-source, its community has grown huge.Now, it’s time for you start to learn in practice how GraphQL works and all about its magic.GraphQL is an open-source query language developed by Facebook. It provides us with a more efficient way design, create, and consume our APIs. Basically, it’s the replacement for REST.GraphQL has a lot of features, like:You write the data that you want, and you get exactly the data that you want. No more over-fetching of information as we are used to with REST.It gives us a single endpoint, no more version 2 or version 3 for the same API.GraphQL is strongly-typed, and with that you can validate a query within the GraphQL type system before execution. It helps us build more powerful APIs.This is a basic introduction to GraphQL — why it’s so powerful and why it’s gaining a lot of popularity these days. If you want to learn more about it, I recommend you to go the GraphQL website and check it out.The main objective in this article is not to learn how to set up a GraphQL server, so we’re not getting deep into that for now. The objective is to learn how GraphQL works in practice, so we’re gonna use a zero-configuration GraphQL server called ☄️ Graphpack.To start our project, we’re going to create a new folder and you can name it whatever you want. I’m going to name it graphql-server:Open your terminal and type:Now, you should have npm or yarn installed in your machine. If you don’t know what these are, npm and yarn are package managers for the JavaScript programming language. For Node.js, the default package manager is npm.Inside your created folder type the following command:Or if you use yarn:npm will create a package.json file for you, and all the dependencies that you installed and your commands will be there.So now, we’re going to install the only dependency that we’re going to use.☄️Graphpack lets you create a GraphQL server with zero configuration. Since we’re just starting with GraphQL, this will help us a lot to go on and learn more without getting worried about a server configuration.In your terminal, inside your root folder, install it like this:Or, if you use yarn, you should go like this:After Graphpack is installed, go to our scripts in package.json file, and put the following code there:We’re going to create a folder called src, and it’s going to be the only folder in our entire server.Create a folder called src, after that, inside our folder, we’re going to create three files only.Inside our src folder create a file called schema.graphql. Inside this first file, put the following code:In this schema.graphql file is going to be our entire GraphQL schema. If you don’t know what it is, I’ll explain later — don't worry.Now, inside our src folder, create a second file. Call it resolvers.js and, inside this second file, put the following code:This resolvers.js file is going to be the way we provide the instructions for turning a GraphQL operation into data.And finally, inside your src folder, create a third file. Call this db.js and, inside this third file, put the following code:In this tutorial we’re not using a real-world database. So this db.js file is going to simulate a database, just for learning purposes.Now our src folder should look like this:Now, if you run the command npm run dev or, if you’re using yarn, yarn dev, you should see this output in your terminal:You can now go to localhost:4000. This means that we’re ready to go and start writing our first queries, mutations, and subscriptions in GraphQL.You see the GraphQL Playground, a powerful GraphQL IDE for better development workflows. If you want to learn more about GraphQL Playground, click here.GraphQL has its own type of language that’s used to write schemas. This is a human-readable schema syntax called Schema Definition Language (SDL). The SDL will be the same, no matter what technology you’re using — you can use this with any language or framework that you want.This schema language its very helpful because it’s simple to understand what types your API is going to have. You can understand it just by looking right it.Types are one of the most important features of GraphQL. Types are custom objects that represent how your API is going to look. For example, if you’re building a social media application, your API should have types such as Posts, Users, Likes, Groups.Types have fields, and these fields return a specific type of data. For example, we’re going to create a User type, we should have some name, email, and age fields. Type fields can be anything, and always return a type of data as Int, Float, String, Boolean, ID, a List of Object Types, or Custom Objects Types.So now to write our first Type, go to your schema.graphql file and replace the type Query that is already there with the following:Each User is going to have an ID, so we gave it an ID type. User is also going to have a name and email, so we gave it a String type, and an age, which we gave an Int type. Pretty simple, right?But, what about those ! at the end of every line? The exclamation point means that the fields are non-nullable, which means that every field must return some data in each query. The only nullable field that we’re going to have in our User type will be age.In GraphQL, you will deal with three main concepts:queries — the way you’re going to get data from the server.mutations — the way you’re going to modify data on the server and get updated data back (create, update, delete).subscriptions — the way you’re going to maintain a real-time connection with the server.I’m going to explain all of them to you. Let’s start with Queries.To explain this in a simple way, queries in GraphQL are how you’re going to get data. One of the most beautiful things about queries in GraphQL is that you are just going to get the exact data that you want. No more, no less. This has a huge positive impact in our API — no more over-fetching or under-fetching information as we had with REST APIs.We’re going to create our first type Query in GraphQL. All our queries will end up inside this type. So to start, we’ll go to our schema.graphql and write a new type called Query:It’s very simple: the users query will return to us an array of one or more Users. It will not return null, because we put in the !, which means it’s a non-nullable query. It should always return something.But we could also return a specific user. For that we’re going to create a new query called user. Inside our Query type, put the following code:Now our Query type should look like this:As you see, with queries in GraphQL we can also pass arguments. In this case, to query for a specific user, we’re going to pass its ID.But, you may be wondering: how does GraphQL know where get the data? That’s why we should have a resolvers.js file. That file tells GraphQL how and where it's going to fetch the data.First, go to our resolvers.js file and import the db.js that we just created a few moments ago. Your resolvers.js file should look like this:Now, we’re going to create our first Query. Go to your resolvers.js file and replace the hello function. Now your Query type should look like this:Now, to explain how is it going to work:Each query resolver has four arguments. In the user function, we’re going to pass id as an argument, and then return the specific user that matches the passed id. Pretty simple.In the users function, we’re just going to return the users array that already exists. It’ll always return to us all of our users.Now, we’re going to test if our queries are working fine. Go to localhost:4000 and put in the following code:It should return to you all of our users.Or, if you want to return a specific user:Now, we’re going to start learning about mutations, one of the most important features in GraphQL.In GraphQL, mutations are the way you’re going to modify data on the server and get updated data back. You can think like the CUD (Create, Update, Delete) of REST .We’re going to create our first type mutation in GraphQL, and all our mutations will end up inside this type. So, to start, go to our schema.graphql and write a new type called mutation:As you can see, we’re going to have three mutations:createUser: we should pass an ID, name, email, and age. It should return a new user to us.updateUser: we should pass an ID, and a new name, email, or age. It should return a new user to us.deleteUser: we should pass an ID. It should return the deleted user to us.Now, go to our resolvers.js file and below the Query object, create a new mutation object like this:Now, our resolvers.js file should look like this:Now, we’re going to test if our mutations are working fine. Go to localhost:4000 and put in the following code:It should return a new user to you. If you want to try making new mutations, I recommend you to try for yourself! Try to delete this same user that you created to see if it’s working fine.Finally, we’re going to start learning about subscriptions, and why they are so powerful.As I said before, subscriptions are the way you’re going to maintain a real-time connection with a server. That means that whenever an event occurs in the server and whenever that event is called, the server will send the corresponding data to the client.By working with subscriptions, you can keep your app updated to the latest changes between different users.A basic subscription is like this:You will say it’s very similar to a query, and yes it is. But it works differently.When something is updated in the server, the server will run the GraphQL query specified in the subscription, and send a newly updated result to the client.We’re not going to work with subscriptions in this specific article, but if you want to read more about them click here.As you have seen, GraphQL is a new technology that is really powerful. It gives us real power to build better and well-designed APIs. That’s why I recommend you start to learn it now. For me, it will eventually replace REST.Thanks for reading the article, please give a comment below!🐦 Follow me on Twitter! ⭐ Follow me on GitHub!",A Beginner’s Guide to GraphQL,2019-01-05T11:45:31Z
https://dev.to/azure/learn-how-you-can-build-a-serverless-graphql-api-on-top-of-a-microservice-architecture-233g,"Follow me on Twitter, happy to take your suggestions on topics or improvements /ChrisThe idea with this article is to show how we can build microservices, dockerize them and combine them in a GraphQL API and query it from a Serverless function, how's that for a lot of buzzwords in one? ;) Microservices, Docker, GraphQL, ServerlessThis is part of series:So, it's quite ambitious to create Microservices, Serverless and deploy to the Cloud in one article so this is a two-parter. This is part one. This part deals with Microservices and GraphQL. In part two we make it serverless and deploy it.In this article we will cover:We will throw you in head first using Docker, GraphQL and some Serverless with Azure functions. This article is more of a recipe of what you can do with the above techniques so if you feel you need a primer on the above here is a list of posts I've written:A library and paradigm like GraphQL, is the most useful when it is able to combine different data sources into one and serve that up as one unified API. A developer of the Front end app can then query the data they need by using just one request.Today it becomes more common to break down a monolithic architecture into microservices, thereby you get many small APIs that works independently. GraphQL and microservices are two paradigms that go really well together. How you wonder? GraphQL is really good at describing schemas but also stitch together different APIs and the end result is something that's really useful for someone building an app as querying for data will be very simple.Different APIs is exactly what we have when we have a Microservices architecture. Using GraphQL on top of it all means we can reap the benefits from our chosen architecture at the same time as an App can get exactly the data it needs.How you wonder? Stay with me throughout this article and you will see exactly how. Bring up your code editor cause you will build it with me :)Ok, so what are we building? It's always grateful to use an e-commerce company as the target as it contains so many interesting problems for us to solve. We will zoom in on two topics in particular namely products and reviews. When it comes to products we need a way to keep track of what products we sell and all their metadata. For reviews we need a way to offer our customer a way to review our products, give it a grade, a comment and so on and so forth. These two concepts can be seen as two isolated islands that can be maintained and developed independently. If for example, a product gets a new description there is no reason that should affect the review of said product.Ok, so we turn these two concepts into product service and a review service.What does the data look like for these services? Well they are in their infancy so let's assume the product service is a list of products for now with a product looking something like this:The review service would also hold data in a list like so:As you can see from the above data description the review service holds a reference to a product in the product service and it's by querying the product service that you get the full picture of both the review and the product involved.Ok, so we understand what the services need to provide in terms of a schema. The services also need to be containerized so we will describe how to build them using Docker a Dockerfile and Docker Compose.So the GraphQL API serves as this high-level API that is able to combine results from our product service as well as review service. It's schema should look something like this:We assume that when a user of our GraphQL API queries for Reviews they want to see more than just the review but also some extra data on the Product, what's it called, what it is and so on. For that reason, we've added the product property on the Review type in the above schema so that when we drill down in our query, we are able to get both Review and Product information.So where does Serverless come into this? We need a way to host our API. We could be using an App Service but because our GraphQL API doesn't need to hold any state of its own and it only does a computation (it assembles a result) it makes more sense to make it a light-weight on-demand Azure Function. So that's what we are going to do :) As stated in the beginning we are saving this for the second part of our series, we don't want to bore you by a too lengthy article :)We opt for making these services as simple as possible so we create REST APIs using Node.js and Express, like so:The app.js file for /products looks like this:and the app.js for /reviews looks like this:Looks almost the same right? Well, we try to keep things simple for now and return static data but it's quite simple to add database later on.Before we start Dockerizing we need to install our dependency Express like so:This needs to be done for each service.Ok, we showed you in the directory for each service how there was a Dockerfile. It looks like this:Let's go up one level and create a docker-compose.yaml file, so it's easier to create our images and containers. Your file system should now look like this:Your docker-compose.yaml should have the following content:We can now get our service up and running withI always feel like I'm starting up a jet engine when I run this command as all of my containers go up at the same time, so here goes, ignition :)You should be able to find the products service at http://localhost:8000 and the reviews service at http://localhost:8001. That covers the microservices for now, let's build our GraphQL API next.Your products service should look like the following:and your review service should look like this:There are many ways to build a GraphQL server, we could be using the raw graphql NPM library or the express-graphql, this will host our server in a Node.js Express server. Or we could be using the one from Apollo and so on. We opt for the first one graphql as we will ultimately serve it from a Serverless function.So what do we need to do:Now, this is an interesting one, we have two options here for defining a schema, either use the helper function buildSchema() or use the raw approach and construct our schema using primitives. For this case, we will use the raw approach and the reason for that is I simply couldn't find how to resolve things at depth using buildSchema() despite reading through the manual twice. It's strangely enough easily done if we were to use express-graphql or Apollo so sorry if you feel your eyes bleed a little ;)Ok, let's define our schema first:Above we are defining two types Review and Product and we expose two query fields products and reviews.I want you to pay special attention to the variable reviewType and how we resolve the product field. Here we are resolving it like so:Why do we do that? Well, it has to do with how data is stored on a Review. Let's revisit that. A Review stores its data like so:As you can see above the product field is an integer. It's a foreign key pointing to a real product in the product service. So we need to resolve it so the API can be queried like so:If we don't resolve product to a product object instead the above query would error out.In our schema.js we called methods like getProducts(), getReviews() and getProduct() and we need those to exist so we create a file services.js, like so:Ok, we can see above that methods getProducts() and getReviews() makes HTTP requests to URL, at least judging by the names process.env.PRODUCTS_URL and process.env.REVIEW_URL. For now, we have created a .env file in which we create those two env variables like so:Wait, isn't that? Yes, it is. It is the URLs to product service and review service after we used docker-compose to bring them up. This is a great way to test your Microservice architecture locally but also prepare for deployment to the Cloud. Deploying to the Cloud is almost as simple as switching these env variables to Cloud endpoints, as you will see in the next part of this article series :)Ok, so we need to try our code out. To do that let's create an app.js file in which we invoke the graphql() function and let's provide it our schema and query, like so:In the above code, we specify a query and we expect the fields hello, products and reviews to come back to us and finally we invoke graphql() that on the then() callback serves up the result. The result should look like this:We set out on a journey that would eventually lead us to the cloud. We are not there yet but part two will take us all the way. In this first part, we've managed to create microservices and dockerize them. Furthermore, we've managed to construct a GraphQL API that is able to define a schema that merges our two APIs together and serve that up.What remains to do, that will be the job of the second part, is to push our containers to the cloud and create service endpoints. When we have the service endpoints we can replace the value of environment variables to use the Cloud URLs instead of the localhost ones we are using now.Lastly, we need to scaffold a serverless function but that's all in the next part so I hope you look forward to that :)","Learn how YOU can build a Serverless GraphQL API on top of a Microservice architecture, part I",2019-04-13T12:22:00Z
https://dev.to/qainsights/performance-testing-neo4j-database-using-bolt-protocol-in-apache-jmeter-1oa9,"Apache JMeter 5.2 has been released with lots of new features, enhancements, and bug fixes. JMeter 5.2 supports Bolt protocol out of the box. No need to install it via Plugins Manager. This blog post provides you an overview about performance testing Neo4j database using Bolt protocol in Apache JMeter 5.2First, you need to add the basic elements such as Thread Group, Bolt Connection Configuration, and View Results Tree, then you need to add Bolt Request.In Bolt Connection Configuration, you need to add the Bolt URI with the valid credentials. Default credentials of Neo4j is neo4j/neo4j. At first login, it will prompt you to change the password.Once the connection is set up, next you need to load up the demo data in your Neo4j.Detailed instructions are in here.Bolt communicates over 7687 port.Neo4j can be accessed from your favorite browser using http://localhost:7474.Now, head back to JMeter and enter the below command in the Bolt Request.MATCH (n:Movie) RETURN n LIMIT 25Make sure you set the Record Query Results flag set to true, and then hit run.In View Results Tree, you can see the response as shown below.You cannot configure the pool size of Bolt Connection in this inception implementation.You need to set the Record Query Results set to false for better performance.Here is the complete video demo.This repo has the sample JMeter test plan to demonstrate Bolt request. For detailed instruction, please visit my blogBuy me a tea",Performance Testing Neo4j Database using Bolt Protocol in Apache JMeter,2019-11-08T17:09:37Z
